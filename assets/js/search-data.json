{
  
    
        "post0": {
            "title": "Import Relevant Libraries",
            "content": "import numpy as np import matplotlib.pyplot as plt import matplotlib.colors as mcolors import pandas as pd import random import math import time from sklearn.linear_model import LinearRegression, BayesianRidge from sklearn.model_selection import RandomizedSearchCV, train_test_split from sklearn.preprocessing import PolynomialFeatures from sklearn.svm import SVR from sklearn.metrics import mean_squared_error, mean_absolute_error import datetime import operator plt.style.use(&#39;fivethirtyeight&#39;) %matplotlib inline import warnings warnings.filterwarnings(&quot;ignore&quot;) . Load the DataFrame from th Sources. . confirmed_df = pd.read_csv(&#39;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv&#39;) deaths_df = pd.read_csv(&#39;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv&#39;) recoveries_df = pd.read_csv(&#39;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv&#39;) latest_data = pd.read_csv(&#39;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/10-07-2020.csv&#39;) us_medical_data = pd.read_csv(&#39;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports_us/10-07-2020.csv&#39;) apple_mobility = pd.read_csv(&#39;https://covid19-static.cdn-apple.com/covid19-mobility-data/2018HotfixDev19/v3/en-us/applemobilitytrends-2020-10-07.csv&#39;) . HTTPError Traceback (most recent call last) &lt;ipython-input-3-e0928034943f&gt; in &lt;module&gt; 4 latest_data = pd.read_csv(&#39;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/10-07-2020.csv&#39;) 5 us_medical_data = pd.read_csv(&#39;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports_us/10-07-2020.csv&#39;) -&gt; 6 apple_mobility = pd.read_csv(&#39;https://covid19-static.cdn-apple.com/covid19-mobility-data/2018HotfixDev19/v3/en-us/applemobilitytrends-2020-10-07.csv&#39;) ~ anaconda3 lib site-packages pandas io parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision) 674 ) 675 --&gt; 676 return _read(filepath_or_buffer, kwds) 677 678 parser_f.__name__ = name ~ anaconda3 lib site-packages pandas io parsers.py in _read(filepath_or_buffer, kwds) 428 # though mypy handling of conditional imports is difficult. 429 # See https://github.com/python/mypy/issues/1297 --&gt; 430 fp_or_buf, _, compression, should_close = get_filepath_or_buffer( 431 filepath_or_buffer, encoding, compression 432 ) ~ anaconda3 lib site-packages pandas io common.py in get_filepath_or_buffer(filepath_or_buffer, encoding, compression, mode) 170 171 if isinstance(filepath_or_buffer, str) and is_url(filepath_or_buffer): --&gt; 172 req = urlopen(filepath_or_buffer) 173 content_encoding = req.headers.get(&#34;Content-Encoding&#34;, None) 174 if content_encoding == &#34;gzip&#34;: ~ anaconda3 lib site-packages pandas io common.py in urlopen(*args, **kwargs) 139 import urllib.request 140 --&gt; 141 return urllib.request.urlopen(*args, **kwargs) 142 143 ~ anaconda3 lib urllib request.py in urlopen(url, data, timeout, cafile, capath, cadefault, context) 220 else: 221 opener = _opener --&gt; 222 return opener.open(url, data, timeout) 223 224 def install_opener(opener): ~ anaconda3 lib urllib request.py in open(self, fullurl, data, timeout) 529 for processor in self.process_response.get(protocol, []): 530 meth = getattr(processor, meth_name) --&gt; 531 response = meth(req, response) 532 533 return response ~ anaconda3 lib urllib request.py in http_response(self, request, response) 638 # request was successfully received, understood, and accepted. 639 if not (200 &lt;= code &lt; 300): --&gt; 640 response = self.parent.error( 641 &#39;http&#39;, request, response, code, msg, hdrs) 642 ~ anaconda3 lib urllib request.py in error(self, proto, *args) 567 if http_err: 568 args = (dict, &#39;default&#39;, &#39;http_error_default&#39;) + orig_args --&gt; 569 return self._call_chain(*args) 570 571 # XXX probably also want an abstract factory that knows when it makes ~ anaconda3 lib urllib request.py in _call_chain(self, chain, kind, meth_name, *args) 500 for handler in handlers: 501 func = getattr(handler, meth_name) --&gt; 502 result = func(*args) 503 if result is not None: 504 return result ~ anaconda3 lib urllib request.py in http_error_default(self, req, fp, code, msg, hdrs) 647 class HTTPDefaultErrorHandler(BaseHandler): 648 def http_error_default(self, req, fp, code, msg, hdrs): --&gt; 649 raise HTTPError(req.full_url, code, msg, hdrs, fp) 650 651 class HTTPRedirectHandler(BaseHandler): HTTPError: HTTP Error 404: Not Found . confirmed_df.head() . Province/State Country/Region Lat Long 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 ... 11/3/20 11/4/20 11/5/20 11/6/20 11/7/20 11/8/20 11/9/20 11/10/20 11/11/20 11/12/20 . 0 NaN | Afghanistan | 33.93911 | 67.709953 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 41728 | 41814 | 41935 | 41975 | 42033 | 42092 | 42297 | 42463 | 42609 | 42795 | . 1 NaN | Albania | 41.15330 | 20.168300 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 21904 | 22300 | 22721 | 23210 | 23705 | 24206 | 24731 | 25294 | 25801 | 26211 | . 2 NaN | Algeria | 28.03390 | 1.659600 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 58979 | 59527 | 60169 | 60800 | 61381 | 62051 | 62693 | 63446 | 64257 | 65108 | . 3 NaN | Andorra | 42.50630 | 1.521800 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 4910 | 5045 | 5135 | 5135 | 5319 | 5383 | 5437 | 5477 | 5567 | 5616 | . 4 NaN | Angola | -11.20270 | 17.873900 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 11577 | 11813 | 12102 | 12223 | 12335 | 12433 | 12680 | 12816 | 12953 | 13053 | . 5 rows × 300 columns . latest_data.head() . FIPS Admin2 Province_State Country_Region Last_Update Lat Long_ Confirmed Deaths Recovered Active Combined_Key Incidence_Rate Case-Fatality_Ratio . 0 NaN | NaN | NaN | Afghanistan | 2020-10-08 04:23:56 | 33.93911 | 67.709953 | 39548 | 1469 | 33045 | 5034.0 | Afghanistan | 101.591794 | 3.714474 | . 1 NaN | NaN | NaN | Albania | 2020-10-08 04:23:56 | 41.15330 | 20.168300 | 14730 | 407 | 9115 | 5208.0 | Albania | 511.849329 | 2.763069 | . 2 NaN | NaN | NaN | Algeria | 2020-10-08 04:23:56 | 28.03390 | 1.659600 | 52520 | 1771 | 36857 | 13892.0 | Algeria | 119.769101 | 3.372049 | . 3 NaN | NaN | NaN | Andorra | 2020-10-08 04:23:56 | 42.50630 | 1.521800 | 2568 | 53 | 1715 | 800.0 | Andorra | 3323.626480 | 2.063863 | . 4 NaN | NaN | NaN | Angola | 2020-10-08 04:23:56 | -11.20270 | 17.873900 | 5725 | 211 | 2598 | 2916.0 | Angola | 17.419075 | 3.685590 | . us_medical_data.head() . Province_State Country_Region Last_Update Lat Long_ Confirmed Deaths Recovered Active FIPS Incident_Rate People_Tested People_Hospitalized Mortality_Rate UID ISO3 Testing_Rate Hospitalization_Rate . 0 Alabama | US | 2020-10-08 04:30:35 | 32.3182 | -86.9023 | 161418 | 2601 | 67948.0 | 90869.0 | 1.0 | 3292.105030 | 1180818.0 | NaN | 1.611344 | 84000001 | USA | 24082.672793 | NaN | . 1 Alaska | US | 2020-10-08 04:30:35 | 61.3707 | -152.4044 | 8878 | 59 | 5626.0 | 3193.0 | 2.0 | 1213.595883 | 490074.0 | NaN | 0.664564 | 84000002 | USA | 66991.640979 | NaN | . 2 American Samoa | US | 2020-10-08 04:30:35 | -14.2710 | -170.1320 | 0 | 0 | NaN | 0.0 | 60.0 | 0.000000 | 1616.0 | NaN | NaN | 16 | ASM | 2904.333136 | NaN | . 3 Arizona | US | 2020-10-08 04:30:35 | 33.7298 | -111.4312 | 222538 | 5733 | 36336.0 | 180469.0 | 4.0 | 3057.379480 | 1518694.0 | NaN | 2.576189 | 84000004 | USA | 20864.858463 | NaN | . 4 Arkansas | US | 2020-10-08 04:30:35 | 34.9697 | -92.3731 | 88880 | 1482 | 80703.0 | 6695.0 | 5.0 | 2945.187958 | 1087671.0 | NaN | 1.667417 | 84000005 | USA | 36041.803908 | NaN | . cols = confirmed_df.keys() . confirmed = confirmed_df.loc[:, cols[4]:cols[-1]] deaths = deaths_df.loc[:, cols[4]:cols[-1]] recoveries = recoveries_df.loc[:, cols[4]:cols[-1]] . dates = confirmed.keys() world_cases = [] total_deaths = [] mortality_rate = [] recovery_rate = [] total_recovered = [] total_active = [] for i in dates: confirmed_sum = confirmed[i].sum() death_sum = deaths[i].sum() recovered_sum = recoveries[i].sum() # confirmed, deaths, recovered, and active world_cases.append(confirmed_sum) total_deaths.append(death_sum) total_recovered.append(recovered_sum) total_active.append(confirmed_sum-death_sum-recovered_sum) # calculate rates mortality_rate.append(death_sum/confirmed_sum) recovery_rate.append(recovered_sum/confirmed_sum) . Getting daily increase and moving averages. . def daily_increase(data): d = [] for i in range(len(data)): if i == 0: d.append(data[0]) else: d.append(data[i]-data[i-1]) return d def moving_average(data, window_size): moving_average = [] for i in range(len(data)): if i + window_size &lt; len(data): moving_average.append(np.mean(data[i:i+window_size])) else: moving_average.append(np.mean(data[i:len(data)])) return moving_average # window size window = 7 # confirmed cases world_daily_increase = daily_increase(world_cases) world_confirmed_avg= moving_average(world_cases, window) world_daily_increase_avg = moving_average(world_daily_increase, window) # deaths world_daily_death = daily_increase(total_deaths) world_death_avg = moving_average(total_deaths, window) world_daily_death_avg = moving_average(world_daily_death, window) # recoveries world_daily_recovery = daily_increase(total_recovered) world_recovery_avg = moving_average(total_recovered, window) world_daily_recovery_avg = moving_average(world_daily_recovery, window) # active world_active_avg = moving_average(total_active, window) . days_since_1_22 = np.array([i for i in range(len(dates))]).reshape(-1, 1) world_cases = np.array(world_cases).reshape(-1, 1) total_deaths = np.array(total_deaths).reshape(-1, 1) total_recovered = np.array(total_recovered).reshape(-1, 1) . Future Forecasting . days_in_future = 10 future_forcast = np.array([i for i in range(len(dates)+days_in_future)]).reshape(-1, 1) adjusted_dates = future_forcast[:-10] . Converting Integer into Datatime for better Visualization . start = &#39;1/22/2020&#39; start_date = datetime.datetime.strptime(start, &#39;%m/%d/%Y&#39;) future_forcast_dates = [] for i in range(len(future_forcast)): future_forcast_dates.append((start_date + datetime.timedelta(days=i)).strftime(&#39;%m/%d/%Y&#39;)) . Slightly modify the data to fit the model better (regression models cannot pick the pattern) . X_train_confirmed, X_test_confirmed, y_train_confirmed, y_test_confirmed = train_test_split(days_since_1_22[50:], world_cases[50:], test_size=0.05, shuffle=False) . Model for predicting number of confirmed cases. I am using support vector machine, bayesian ridge , and linear regression . # c = [0.01, 0.1, 1] # gamma = [0.01, 0.1, 1] # epsilon = [0.01, 0.1, 1] # shrinking = [True, False] # degree = [3, 4, 5] # svm_grid = {&#39;C&#39;: c, &#39;gamma&#39; : gamma, &#39;epsilon&#39;: epsilon, &#39;shrinking&#39; : shrinking, &#39;degree&#39;: degree} # svm = SVR(kernel=&#39;poly&#39;) # svm_search = RandomizedSearchCV(svm, svm_grid, scoring=&#39;neg_mean_squared_error&#39;, cv=3, return_train_score=True, n_jobs=-1, n_iter=30, verbose=1) # svm_search.fit(X_train_confirmed, y_train_confirmed) . . svm_confirmed = SVR(shrinking=True, kernel=&#39;poly&#39;,gamma=0.01, epsilon=1,degree=3, C=0.1) svm_confirmed.fit(X_train_confirmed, y_train_confirmed) svm_pred = svm_confirmed.predict(future_forcast) . Check against testing data . svm_test_pred = svm_confirmed.predict(X_test_confirmed) plt.plot(y_test_confirmed) plt.plot(svm_test_pred) plt.legend([&#39;Test Data&#39;, &#39;SVM Predictions&#39;]) print(&#39;MAE:&#39;, mean_absolute_error(svm_test_pred, y_test_confirmed)) print(&#39;MSE:&#39;,mean_squared_error(svm_test_pred, y_test_confirmed)) . MAE: 2344622.5616368484 MSE: 5523754241193.524 . Transform our data for polynomial regression . poly = PolynomialFeatures(degree=5) poly_X_train_confirmed = poly.fit_transform(X_train_confirmed) poly_X_test_confirmed = poly.fit_transform(X_test_confirmed) poly_future_forcast = poly.fit_transform(future_forcast) bayesian_poly = PolynomialFeatures(degree=5) bayesian_poly_X_train_confirmed = bayesian_poly.fit_transform(X_train_confirmed) bayesian_poly_X_test_confirmed = bayesian_poly.fit_transform(X_test_confirmed) bayesian_poly_future_forcast = bayesian_poly.fit_transform(future_forcast) . Polynomial regression . linear_model = LinearRegression(normalize=True, fit_intercept=False) linear_model.fit(poly_X_train_confirmed, y_train_confirmed) test_linear_pred = linear_model.predict(poly_X_test_confirmed) linear_pred = linear_model.predict(poly_future_forcast) print(&#39;MAE:&#39;, mean_absolute_error(test_linear_pred, y_test_confirmed)) print(&#39;MSE:&#39;,mean_squared_error(test_linear_pred, y_test_confirmed)) . MAE: 1206493.191649382 MSE: 1585980580691.81 . print(linear_model.coef_) . [[-1.87018323e+07 7.19991429e+05 -1.01838910e+04 7.06102955e+01 -2.13579009e-01 2.45409901e-04]] . plt.plot(y_test_confirmed) plt.plot(test_linear_pred) plt.legend([&#39;Test Data&#39;, &#39;Polynomial Regression Predictions&#39;]) . &lt;matplotlib.legend.Legend at 0x162b9686a30&gt; . Bayesian ridge polynomial regression . tol = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2] alpha_1 = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3] alpha_2 = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3] lambda_1 = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3] lambda_2 = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3] normalize = [True, False] bayesian_grid = {&#39;tol&#39;: tol, &#39;alpha_1&#39;: alpha_1, &#39;alpha_2&#39; : alpha_2, &#39;lambda_1&#39;: lambda_1, &#39;lambda_2&#39; : lambda_2, &#39;normalize&#39; : normalize} bayesian = BayesianRidge(fit_intercept=False) bayesian_search = RandomizedSearchCV(bayesian, bayesian_grid, scoring=&#39;neg_mean_squared_error&#39;, cv=3, return_train_score=True, n_jobs=-1, n_iter=40, verbose=1) bayesian_search.fit(bayesian_poly_X_train_confirmed, y_train_confirmed) . Fitting 3 folds for each of 40 candidates, totalling 120 fits . [Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers. [Parallel(n_jobs=-1)]: Done 34 tasks | elapsed: 1.5s [Parallel(n_jobs=-1)]: Done 105 out of 120 | elapsed: 1.5s remaining: 0.1s [Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed: 1.5s finished . RandomizedSearchCV(cv=3, estimator=BayesianRidge(fit_intercept=False), n_iter=40, n_jobs=-1, param_distributions={&#39;alpha_1&#39;: [1e-07, 1e-06, 1e-05, 0.0001, 0.001], &#39;alpha_2&#39;: [1e-07, 1e-06, 1e-05, 0.0001, 0.001], &#39;lambda_1&#39;: [1e-07, 1e-06, 1e-05, 0.0001, 0.001], &#39;lambda_2&#39;: [1e-07, 1e-06, 1e-05, 0.0001, 0.001], &#39;normalize&#39;: [True, False], &#39;tol&#39;: [1e-06, 1e-05, 0.0001, 0.001, 0.01]}, return_train_score=True, scoring=&#39;neg_mean_squared_error&#39;, verbose=1) . bayesian_search.best_params_ . {&#39;tol&#39;: 0.01, &#39;normalize&#39;: True, &#39;lambda_2&#39;: 1e-07, &#39;lambda_1&#39;: 0.001, &#39;alpha_2&#39;: 1e-07, &#39;alpha_1&#39;: 1e-05} . bayesian_confirmed = bayesian_search.best_estimator_ test_bayesian_pred = bayesian_confirmed.predict(bayesian_poly_X_test_confirmed) bayesian_pred = bayesian_confirmed.predict(bayesian_poly_future_forcast) print(&#39;MAE:&#39;, mean_absolute_error(test_bayesian_pred, y_test_confirmed)) print(&#39;MSE:&#39;,mean_squared_error(test_bayesian_pred, y_test_confirmed)) . MAE: 2316291.1614478836 MSE: 5926142585167.471 . plt.plot(y_test_confirmed) plt.plot(test_bayesian_pred) plt.legend([&#39;Test Data&#39;, &#39;Bayesian Ridge Polynomial Predictions&#39;]) . &lt;matplotlib.legend.Legend at 0x162bb75cd00&gt; . Graphing the number of confirmed cases, active cases, deaths, recoveries, mortality rate (CFR), and recovery rate . adjusted_dates = adjusted_dates.reshape(1, -1)[0] plt.figure(figsize=(16, 10)) plt.plot(adjusted_dates, world_cases) plt.plot(adjusted_dates, world_confirmed_avg, linestyle=&#39;dashed&#39;, color=&#39;orange&#39;) plt.title(&#39;# of Coronavirus Cases Over Time&#39;, size=30) plt.xlabel(&#39;Days Since 1/22/2020&#39;, size=30) plt.ylabel(&#39;# of Cases&#39;, size=30) plt.legend([&#39;Worldwide Coronavirus Cases&#39;, &#39;Moving Average {} Days&#39;.format(window)], prop={&#39;size&#39;: 20}) plt.xticks(size=20) plt.yticks(size=20) plt.show() plt.savefig(&quot;corona.png&quot;, bbox_inches=&#39;tight&#39;, dpi=600) . &lt;Figure size 432x288 with 0 Axes&gt; . plt.figure(figsize=(16, 10)) plt.plot(adjusted_dates, total_deaths) plt.plot(adjusted_dates, world_death_avg, linestyle=&#39;dashed&#39;, color=&#39;orange&#39;) plt.title(&#39;# of Coronavirus Deaths Over Time&#39;, size=30) plt.xlabel(&#39;Days Since 1/22/2020&#39;, size=30) plt.ylabel(&#39;# of Cases&#39;, size=30) plt.legend([&#39;Worldwide Coronavirus Deaths&#39;, &#39;Moving Average {} Days&#39;.format(window)], prop={&#39;size&#39;: 20}) plt.xticks(size=20) plt.yticks(size=20) plt.show() . plt.figure(figsize=(16, 10)) plt.plot(adjusted_dates, total_recovered) plt.plot(adjusted_dates, world_recovery_avg, linestyle=&#39;dashed&#39;, color=&#39;orange&#39;) plt.title(&#39;# of Coronavirus Recoveries Over Time&#39;, size=30) plt.xlabel(&#39;Days Since 1/22/2020&#39;, size=30) plt.ylabel(&#39;# of Cases&#39;, size=30) plt.legend([&#39;Worldwide Coronavirus Recoveries&#39;, &#39;Moving Average {} Days&#39;.format(window)], prop={&#39;size&#39;: 20}) plt.xticks(size=20) plt.yticks(size=20) plt.show() . plt.figure(figsize=(16, 10)) plt.plot(adjusted_dates, total_active) plt.plot(adjusted_dates, world_active_avg, linestyle=&#39;dashed&#39;, color=&#39;orange&#39;) plt.title(&#39;# of Coronavirus Active Cases Over Time&#39;, size=30) plt.xlabel(&#39;Days Since 1/22/2020&#39;, size=30) plt.ylabel(&#39;# of Active Cases&#39;, size=30) plt.legend([&#39;Worldwide Coronavirus Active Cases&#39;, &#39;Moving Average {} Days&#39;.format(window)], prop={&#39;size&#39;: 20}) plt.xticks(size=20) plt.yticks(size=20) plt.show() . f=plt.figure(figsize=(16, 10)) plt.bar(adjusted_dates, world_daily_increase) plt.plot(adjusted_dates, world_daily_increase_avg, color=&#39;orange&#39;, linestyle=&#39;dashed&#39;) plt.title(&#39;World Daily Increases in Confirmed Cases&#39;, size=30) plt.xlabel(&#39;Days Since 1/22/2020&#39;, size=30) plt.ylabel(&#39;# of Cases&#39;, size=30) plt.legend([&#39;Moving Average {} Days&#39;.format(window), &#39;World Daily Increase in COVID-19 Cases&#39;], prop={&#39;size&#39;: 20}) plt.xticks(size=20) plt.yticks(size=20) plt.style.use(&#39;dark_background&#39;) plt.show() f.savefig(&quot;corona10.jpg&quot;) . plt.figure(figsize=(16, 10)) plt.bar(adjusted_dates, world_daily_death) plt.plot(adjusted_dates, world_daily_death_avg, color=&#39;orange&#39;, linestyle=&#39;dashed&#39;) plt.title(&#39;World Daily Increases in Confirmed Deaths&#39;, size=30) plt.xlabel(&#39;Days Since 1/22/2020&#39;, size=30) plt.ylabel(&#39;# of Cases&#39;, size=30) plt.legend([&#39;Moving Average {} Days&#39;.format(window), &#39;World Daily Increase in COVID-19 Deaths&#39;], prop={&#39;size&#39;: 20}) plt.xticks(size=20) plt.yticks(size=20) plt.show() . plt.figure(figsize=(16, 10)) plt.bar(adjusted_dates, world_daily_recovery) plt.plot(adjusted_dates, world_daily_recovery_avg, color=&#39;orange&#39;, linestyle=&#39;dashed&#39;) plt.title(&#39;World Daily Increases in Confirmed Recoveries&#39;, size=30) plt.xlabel(&#39;Days Since 1/22/2020&#39;, size=30) plt.ylabel(&#39;# of Cases&#39;, size=30) plt.legend([&#39;Moving Average {} Days&#39;.format(window), &#39;World Daily Increase in COVID-19 Recoveries&#39;], prop={&#39;size&#39;: 20}) plt.xticks(size=20) plt.yticks(size=20) plt.show() . plt.figure(figsize=(16, 10)) plt.plot(adjusted_dates, np.log10(world_cases)) plt.title(&#39;Log of # of Coronavirus Cases Over Time&#39;, size=30) plt.xlabel(&#39;Days Since 1/22/2020&#39;, size=30) plt.ylabel(&#39;# of Cases&#39;, size=30) plt.xticks(size=20) plt.yticks(size=20) plt.show() . plt.figure(figsize=(16, 10)) plt.plot(adjusted_dates, np.log10(total_deaths)) plt.title(&#39;Log of # of Coronavirus Deaths Over Time&#39;, size=30) plt.xlabel(&#39;Days Since 1/22/2020&#39;, size=30) plt.ylabel(&#39;# of Cases&#39;, size=30) plt.xticks(size=20) plt.yticks(size=20) plt.show() . plt.figure(figsize=(16, 10)) plt.plot(adjusted_dates, np.log10(total_recovered)) plt.title(&#39;Log of # of Coronavirus Recoveries Over Time&#39;, size=30) plt.xlabel(&#39;Days Since 1/22/2020&#39;, size=30) plt.ylabel(&#39;# of Cases&#39;, size=30) plt.xticks(size=20) plt.yticks(size=20) plt.show() . def country_plot(x, y1, y2, y3, y4, country): # window is set as 14 in in the beginning of the notebook confirmed_avg = moving_average(y1, window) confirmed_increase_avg = moving_average(y2, window) death_increase_avg = moving_average(y3, window) recovery_increase_avg = moving_average(y4, window) plt.figure(figsize=(16, 10)) plt.plot(x, y1) plt.plot(x, confirmed_avg, color=&#39;red&#39;, linestyle=&#39;dashed&#39;) plt.legend([&#39;{} Confirmed Cases&#39;.format(country), &#39;Moving Average {} Days&#39;.format(window)], prop={&#39;size&#39;: 20}) plt.title(&#39;{} Confirmed Cases&#39;.format(country), size=30) plt.xlabel(&#39;Days Since 1/22/2020&#39;, size=30) plt.ylabel(&#39;# of Cases&#39;, size=30) plt.xticks(size=20) plt.yticks(size=20) plt.show() plt.figure(figsize=(16, 10)) plt.bar(x, y2) plt.plot(x, confirmed_increase_avg, color=&#39;red&#39;, linestyle=&#39;dashed&#39;) plt.legend([&#39;Moving Average {} Days&#39;.format(window), &#39;{} Daily Increase in Confirmed Cases&#39;.format(country)], prop={&#39;size&#39;: 20}) plt.title(&#39;{} Daily Increases in Confirmed Cases&#39;.format(country), size=30) plt.xlabel(&#39;Days Since 1/22/2020&#39;, size=30) plt.ylabel(&#39;# of Cases&#39;, size=30) plt.xticks(size=20) plt.yticks(size=20) plt.show() plt.figure(figsize=(16, 10)) plt.bar(x, y3) plt.plot(x, death_increase_avg, color=&#39;red&#39;, linestyle=&#39;dashed&#39;) plt.legend([&#39;Moving Average {} Days&#39;.format(window), &#39;{} Daily Increase in Confirmed Deaths&#39;.format(country)], prop={&#39;size&#39;: 20}) plt.title(&#39;{} Daily Increases in Deaths&#39;.format(country), size=30) plt.xlabel(&#39;Days Since 1/22/2020&#39;, size=30) plt.ylabel(&#39;# of Cases&#39;, size=30) plt.xticks(size=20) plt.yticks(size=20) plt.show() plt.figure(figsize=(16, 10)) plt.bar(x, y4) plt.plot(x, recovery_increase_avg, color=&#39;red&#39;, linestyle=&#39;dashed&#39;) plt.legend([&#39;Moving Average {} Days&#39;.format(window), &#39;{} Daily Increase in Confirmed Recoveries&#39;.format(country)], prop={&#39;size&#39;: 20}) plt.title(&#39;{} Daily Increases in Recoveries&#39;.format(country), size=30) plt.xlabel(&#39;Days Since 1/22/2020&#39;, size=30) plt.ylabel(&#39;# of Cases&#39;, size=30) plt.xticks(size=20) plt.yticks(size=20) plt.show() . def get_country_info(country_name): country_cases = [] country_deaths = [] country_recoveries = [] for i in dates: country_cases.append(confirmed_df[confirmed_df[&#39;Country/Region&#39;]==country_name][i].sum()) country_deaths.append(deaths_df[deaths_df[&#39;Country/Region&#39;]==country_name][i].sum()) country_recoveries.append(recoveries_df[recoveries_df[&#39;Country/Region&#39;]==country_name][i].sum()) return (country_cases, country_deaths, country_recoveries) def country_visualizations(country_name): country_info = get_country_info(country_name) country_cases = country_info[0] country_deaths = country_info[1] country_recoveries = country_info[2] country_daily_increase = daily_increase(country_cases) country_daily_death = daily_increase(country_deaths) country_daily_recovery = daily_increase(country_recoveries) country_plot(adjusted_dates, country_cases, country_daily_increase, country_daily_death, country_daily_recovery, country_name) . Country Specific Graphs. . countries = [&#39;India&#39;,&#39;US&#39;, &#39;Russia&#39;, &#39;Brazil&#39;, &#39;South Africa&#39;, &#39;China&#39;, &#39;Italy&#39;, &#39;Germany&#39;, &#39;Spain&#39;, &#39;France&#39;, &#39;United Kingdom&#39;, &#39;Peru&#39;, &#39;Mexico&#39;, &#39;Colombia&#39;, &#39;Saudi Arabia&#39;, &#39;Iran&#39;, &#39;Bangladesh&#39;, &#39;Pakistan&#39;, &#39;Turkey&#39;, &#39;Philippines&#39;, &#39;Iraq&#39;, &#39;Indonesia&#39;, &#39;Israel&#39;, &#39;Ukraine&#39;, &#39;Ecuador&#39;, &#39;Bolivia&#39;, &#39;Netherlands&#39;] for country in countries: country_visualizations(country) . Country Comparison . compare_countries = [&#39;India&#39;, &#39;US&#39;, &#39;Brazil&#39;, &#39;Russia&#39;, &#39;South Africa&#39;] graph_name = [&#39;Coronavirus Confirmed Cases&#39;, &#39;Coronavirus Confirmed Deaths&#39;, &#39;Coronavirus Confirmed Recoveries&#39;] for num in range(3): plt.figure(figsize=(16, 10)) for country in compare_countries: plt.plot(get_country_info(country)[num]) plt.legend(compare_countries, prop={&#39;size&#39;: 20}) plt.xlabel(&#39;Days since 3/1&#39;, size=30) plt.ylabel(&#39;# of Cases&#39;, size=30) plt.title(graph_name[num], size=30) plt.xticks(size=20) plt.yticks(size=20) plt.show() . def plot_predictions(x, y, pred, algo_name, color): plt.figure(figsize=(16, 10)) plt.plot(x, y) plt.plot(future_forcast, pred, linestyle=&#39;dashed&#39;, color=color) plt.title(&#39;Worldwide Coronavirus Cases Over Time&#39;, size=30) plt.xlabel(&#39;Days Since 1/22/2020&#39;, size=30) plt.ylabel(&#39;# of Cases&#39;, size=30) plt.legend([&#39;Confirmed Cases&#39;, algo_name], prop={&#39;size&#39;: 20}) plt.xticks(size=20) plt.yticks(size=20) plt.show() . Predictions for confirmed coronavirus cases worldwide . plot_predictions(adjusted_dates, world_cases, svm_pred, &#39;SVM Predictions&#39;, &#39;purple&#39;) . plot_predictions(adjusted_dates, world_cases, linear_pred, &#39;Polynomial Regression Predictions&#39;, &#39;orange&#39;) . plot_predictions(adjusted_dates, world_cases, bayesian_pred, &#39;Bayesian Ridge Regression Predictions&#39;, &#39;green&#39;) . Future predictions using SVM . svm_df = pd.DataFrame({&#39;Date&#39;: future_forcast_dates[-10:], &#39;SVM Predicted # of Confirmed Cases Worldwide&#39;: np.round(svm_pred[-10:])}) svm_df.style.background_gradient(cmap=&#39;Reds&#39;) . Date SVM Predicted # of Confirmed Cases Worldwide . 0 10/10/2020 | 41564298.000000 | . 1 10/11/2020 | 42028255.000000 | . 2 10/12/2020 | 42495753.000000 | . 3 10/13/2020 | 42966806.000000 | . 4 10/14/2020 | 43441428.000000 | . 5 10/15/2020 | 43919632.000000 | . 6 10/16/2020 | 44401431.000000 | . 7 10/17/2020 | 44886840.000000 | . 8 10/18/2020 | 45375870.000000 | . 9 10/19/2020 | 45868537.000000 | . Future predictions using polynomial regression . linear_pred = linear_pred.reshape(1,-1)[0] linear_df = pd.DataFrame({&#39;Date&#39;: future_forcast_dates[-10:], &#39;Polynomial Predicted # of Confirmed Cases Worldwide&#39;: np.round(linear_pred[-10:])}) linear_df.style.background_gradient(cmap=&#39;Reds&#39;) . Date Polynomial Predicted # of Confirmed Cases Worldwide . 0 10/10/2020 | 35561907.000000 | . 1 10/11/2020 | 35742041.000000 | . 2 10/12/2020 | 35916673.000000 | . 3 10/13/2020 | 36085620.000000 | . 4 10/14/2020 | 36248697.000000 | . 5 10/15/2020 | 36405715.000000 | . 6 10/16/2020 | 36556484.000000 | . 7 10/17/2020 | 36700810.000000 | . 8 10/18/2020 | 36838496.000000 | . 9 10/19/2020 | 36969344.000000 | . Future predictions using Bayesian Ridge . bayesian_df = pd.DataFrame({&#39;Date&#39;: future_forcast_dates[-10:], &#39;Bayesian Ridge Predicted # of Confirmed Cases Worldwide&#39;: np.round(bayesian_pred[-10:])}) bayesian_df.style.background_gradient(cmap=&#39;Reds&#39;) . Date Bayesian Ridge Predicted # of Confirmed Cases Worldwide . 0 10/10/2020 | 37208289.000000 | . 1 10/11/2020 | 37515075.000000 | . 2 10/12/2020 | 37822044.000000 | . 3 10/13/2020 | 38129164.000000 | . 4 10/14/2020 | 38436406.000000 | . 5 10/15/2020 | 38743739.000000 | . 6 10/16/2020 | 39051133.000000 | . 7 10/17/2020 | 39358556.000000 | . 8 10/18/2020 | 39665976.000000 | . 9 10/19/2020 | 39973362.000000 | . Mortality Rate (worldwide) susceptible to change . mean_mortality_rate = np.mean(mortality_rate) plt.figure(figsize=(16, 10)) plt.plot(adjusted_dates, mortality_rate, color=&#39;orange&#39;) plt.axhline(y = mean_mortality_rate,linestyle=&#39;--&#39;, color=&#39;black&#39;) plt.title(&#39;Worldwide Mortality Rate of Coronavirus Over Time&#39;, size=30) plt.legend([&#39;mortality rate&#39;, &#39;y=&#39;+str(mean_mortality_rate)], prop={&#39;size&#39;: 20}) plt.xlabel(&#39;Days Since 1/22/2020&#39;, size=30) plt.ylabel(&#39;Case Mortality Rate&#39;, size=30) plt.xticks(size=20) plt.yticks(size=20) plt.show() . Recovery Rate (worldwide) suceptible to change . mean_recovery_rate = np.mean(recovery_rate) plt.figure(figsize=(16, 10)) plt.plot(adjusted_dates, recovery_rate, color=&#39;blue&#39;) plt.axhline(y = mean_recovery_rate,linestyle=&#39;--&#39;, color=&#39;black&#39;) plt.title(&#39;Worldwide Recovery Rate of Coronavirus Over Time&#39;, size=30) plt.legend([&#39;recovery rate&#39;, &#39;y=&#39;+str(mean_recovery_rate)], prop={&#39;size&#39;: 20}) plt.xlabel(&#39;Days Since 1/22/2020&#39;, size=30) plt.ylabel(&#39;Case Recovery Rate&#39;, size=30) plt.xticks(size=20) plt.yticks(size=20) plt.show() . Graphing deaths against recoveries . plt.figure(figsize=(16, 10)) plt.plot(adjusted_dates, total_deaths, color=&#39;r&#39;) plt.plot(adjusted_dates, total_recovered, color=&#39;green&#39;) plt.legend([&#39;death&#39;, &#39;recoveries&#39;], loc=&#39;best&#39;, fontsize=25) plt.title(&#39;Worldwide Coronavirus Cases&#39;, size=30) plt.xlabel(&#39;Days Since 1/22/2020&#39;, size=30) plt.ylabel(&#39;# of Cases&#39;, size=30) plt.xticks(size=20) plt.yticks(size=20) plt.show() . Plotting the number of deaths against the number of recoveries . plt.figure(figsize=(16, 10)) plt.plot(total_recovered, total_deaths) plt.title(&#39;# of Coronavirus Deaths vs. # of Coronavirus Recoveries&#39;, size=30) plt.xlabel(&#39;# of Coronavirus Recoveries&#39;, size=30) plt.ylabel(&#39;# of Coronavirus Deaths&#39;, size=30) plt.xticks(size=20) plt.yticks(size=20) plt.show() . Getting information about countries/regions that have confirmed coronavirus cases . unique_countries = list(latest_data[&#39;Country_Region&#39;].unique()) . country_confirmed_cases = [] country_death_cases = [] country_active_cases = [] country_recovery_cases = [] country_incidence_rate = [] country_mortality_rate = [] no_cases = [] for i in unique_countries: cases = latest_data[latest_data[&#39;Country_Region&#39;]==i][&#39;Confirmed&#39;].sum() if cases &gt; 0: country_confirmed_cases.append(cases) else: no_cases.append(i) for i in no_cases: unique_countries.remove(i) # sort countries by the number of confirmed cases unique_countries = [k for k, v in sorted(zip(unique_countries, country_confirmed_cases), key=operator.itemgetter(1), reverse=True)] for i in range(len(unique_countries)): country_confirmed_cases[i] = latest_data[latest_data[&#39;Country_Region&#39;]==unique_countries[i]][&#39;Confirmed&#39;].sum() country_death_cases.append(latest_data[latest_data[&#39;Country_Region&#39;]==unique_countries[i]][&#39;Deaths&#39;].sum()) country_recovery_cases.append(latest_data[latest_data[&#39;Country_Region&#39;]==unique_countries[i]][&#39;Recovered&#39;].sum()) country_active_cases.append(latest_data[latest_data[&#39;Country_Region&#39;]==unique_countries[i]][&#39;Active&#39;].sum()) country_incidence_rate.append(latest_data[latest_data[&#39;Country_Region&#39;]==unique_countries[i]][&#39;Incidence_Rate&#39;].sum()) country_mortality_rate.append(country_death_cases[i]/country_confirmed_cases[i]) . Data table . country_df = pd.DataFrame({&#39;Country Name&#39;: unique_countries, &#39;Number of Confirmed Cases&#39;: country_confirmed_cases, &#39;Number of Deaths&#39;: country_death_cases, &#39;Number of Recoveries&#39; : country_recovery_cases, &#39;Number of Active Cases&#39; : country_active_cases, &#39;Incidence Rate&#39; : country_incidence_rate, &#39;Mortality Rate&#39;: country_mortality_rate}) # number of cases per country/region country_df.style.background_gradient(cmap=&#39;Oranges&#39;) . Country Name Number of Confirmed Cases Number of Deaths Number of Recoveries Number of Active Cases Incidence Rate Mortality Rate . 0 US | 7549682 | 211801 | 2999895 | 4337988.000000 | 6591745.216429 | 0.028054 | . 1 India | 6835655 | 105526 | 5827704 | 902425.000000 | 23997.512161 | 0.015438 | . 2 Brazil | 5000694 | 148228 | 4457172 | 395294.000000 | 86700.387291 | 0.029641 | . 3 Russia | 1242258 | 21755 | 991277 | 229226.000000 | 73285.074851 | 0.017512 | . 4 Colombia | 877684 | 27180 | 773973 | 76531.000000 | 49362.211131 | 0.030968 | . 5 Argentina | 840915 | 22226 | 670725 | 147964.000000 | 1860.605251 | 0.026431 | . 6 Spain | 835901 | 32562 | 150376 | 652963.000000 | 32514.575797 | 0.038954 | . 7 Peru | 832929 | 32914 | 723606 | 76409.000000 | 62429.131575 | 0.039516 | . 8 Mexico | 799188 | 82726 | 679693 | 36769.000000 | 20739.385260 | 0.103513 | . 9 France | 693603 | 32463 | 102061 | 559021.000000 | 11206.745526 | 0.046803 | . 10 South Africa | 685155 | 17248 | 618127 | 49780.000000 | 1155.235430 | 0.025174 | . 11 United Kingdom | 546952 | 42605 | 2425 | 501922.000000 | 8642.449429 | 0.077895 | . 12 Iran | 483844 | 27658 | 397109 | 59077.000000 | 576.053089 | 0.057163 | . 13 Chile | 474440 | 13090 | 447053 | 14298.000000 | 36175.766845 | 0.027590 | . 14 Iraq | 391044 | 9604 | 319784 | 61656.000000 | 972.202053 | 0.024560 | . 15 Bangladesh | 373151 | 5440 | 286631 | 81080.000000 | 226.578662 | 0.014579 | . 16 Saudi Arabia | 337711 | 4947 | 323208 | 9556.000000 | 970.047366 | 0.014649 | . 17 Italy | 333940 | 36061 | 235303 | 62576.000000 | 11033.927138 | 0.107986 | . 18 Philippines | 329637 | 5925 | 273723 | 49989.000000 | 300.815602 | 0.017974 | . 19 Turkey | 329138 | 8609 | 288954 | 31575.000000 | 390.255681 | 0.026156 | . 20 Pakistan | 316934 | 6544 | 302375 | 8015.000000 | 1935.790852 | 0.020648 | . 21 Indonesia | 315714 | 11472 | 240291 | 63951.000000 | 115.424766 | 0.036337 | . 22 Germany | 311137 | 9582 | 269722 | 31833.000000 | 4970.416123 | 0.030797 | . 23 Israel | 281481 | 1824 | 216613 | 63044.000000 | 3252.032426 | 0.006480 | . 24 Ukraine | 245698 | 4707 | 109968 | 131023.000000 | 15849.050872 | 0.019158 | . 25 Canada | 175380 | 9593 | 147814 | 17974.000000 | 2601.745100 | 0.054698 | . 26 Netherlands | 155465 | 6574 | 4607 | 144284.000000 | 14471.340177 | 0.042286 | . 27 Ecuador | 143531 | 11743 | 120511 | 11277.000000 | 813.526678 | 0.081815 | . 28 Romania | 142570 | 5203 | 111564 | 25803.000000 | 741.097602 | 0.036494 | . 29 Morocco | 140024 | 2439 | 118142 | 19443.000000 | 379.360290 | 0.017418 | . 30 Belgium | 137868 | 10108 | 19895 | 107865.000000 | 1189.582122 | 0.073317 | . 31 Bolivia | 137706 | 8192 | 99268 | 30246.000000 | 1179.693805 | 0.059489 | . 32 Qatar | 127181 | 218 | 124108 | 2855.000000 | 4414.382207 | 0.001714 | . 33 Panama | 117300 | 2448 | 93610 | 21242.000000 | 2718.570268 | 0.020870 | . 34 Dominican Republic | 116148 | 2159 | 92157 | 21832.000000 | 1070.695316 | 0.018588 | . 35 Kuwait | 108743 | 639 | 100776 | 7328.000000 | 2546.338738 | 0.005876 | . 36 Kazakhstan | 108454 | 1746 | 103604 | 3104.000000 | 577.598617 | 0.016099 | . 37 Poland | 107319 | 2792 | 75346 | 29181.000000 | 283.563083 | 0.026016 | . 38 Egypt | 104035 | 6010 | 97492 | 533.000000 | 101.661804 | 0.057769 | . 39 Oman | 103465 | 1000 | 91329 | 11136.000000 | 2026.094745 | 0.009665 | . 40 United Arab Emirates | 101840 | 436 | 91710 | 9694.000000 | 1029.685351 | 0.004281 | . 41 Sweden | 96677 | 5892 | 0 | 90785.000000 | 18122.012006 | 0.060945 | . 42 Guatemala | 95704 | 3335 | 84036 | 8333.000000 | 534.194648 | 0.034847 | . 43 Czechia | 95360 | 829 | 50767 | 43764.000000 | 890.467460 | 0.008693 | . 44 Nepal | 94253 | 578 | 68668 | 25007.000000 | 323.484302 | 0.006132 | . 45 China | 90687 | 4739 | 85588 | 360.000000 | 232.462721 | 0.052257 | . 46 Japan | 87039 | 1614 | 79123 | 6303.000000 | 1850.902203 | 0.018543 | . 47 Costa Rica | 83497 | 1024 | 50295 | 32178.000000 | 1639.087779 | 0.012264 | . 48 Belarus | 81505 | 874 | 75683 | 4948.000000 | 862.548748 | 0.010723 | . 49 Portugal | 81256 | 2040 | 51037 | 28179.000000 | 796.884720 | 0.025106 | . 50 Honduras | 81016 | 2466 | 30590 | 47960.000000 | 817.962710 | 0.030438 | . 51 Ethiopia | 80895 | 1255 | 35670 | 43970.000000 | 70.365761 | 0.015514 | . 52 Venezuela | 80404 | 671 | 71531 | 8202.000000 | 282.754822 | 0.008345 | . 53 Bahrain | 73932 | 262 | 69411 | 4259.000000 | 4344.895312 | 0.003544 | . 54 Nigeria | 59738 | 1113 | 51403 | 7222.000000 | 28.979392 | 0.018631 | . 55 Uzbekistan | 59579 | 491 | 56165 | 2923.000000 | 178.011431 | 0.008241 | . 56 Moldova | 58794 | 1406 | 42480 | 14908.000000 | 1457.474945 | 0.023914 | . 57 Singapore | 57840 | 27 | 57624 | 189.000000 | 988.659981 | 0.000467 | . 58 Switzerland | 57709 | 2082 | 47300 | 8327.000000 | 666.800083 | 0.036078 | . 59 Armenia | 53755 | 995 | 45110 | 7650.000000 | 1814.065308 | 0.018510 | . 60 Algeria | 52520 | 1771 | 36857 | 13892.000000 | 119.769101 | 0.033720 | . 61 Austria | 50848 | 830 | 40499 | 9519.000000 | 564.576301 | 0.016323 | . 62 Lebanon | 48377 | 433 | 21120 | 26824.000000 | 708.774611 | 0.008951 | . 63 Kyrgyzstan | 48097 | 1069 | 43798 | 3230.000000 | 737.210177 | 0.022226 | . 64 Ghana | 46829 | 303 | 46060 | 466.000000 | 150.706668 | 0.006470 | . 65 Paraguay | 46435 | 989 | 29270 | 16176.000000 | 651.031261 | 0.021299 | . 66 West Bank and Gaza | 42840 | 355 | 35953 | 6532.000000 | 839.766841 | 0.008287 | . 67 Azerbaijan | 41113 | 602 | 38858 | 1653.000000 | 405.486640 | 0.014643 | . 68 Kenya | 39907 | 748 | 31659 | 7500.000000 | 74.216171 | 0.018744 | . 69 Ireland | 39584 | 1816 | 23364 | 14404.000000 | 801.653207 | 0.045877 | . 70 Afghanistan | 39548 | 1469 | 33045 | 5034.000000 | 101.591794 | 0.037145 | . 71 Libya | 39513 | 608 | 22831 | 16074.000000 | 575.045112 | 0.015387 | . 72 Serbia | 34193 | 758 | 0 | 33435.000000 | 391.342017 | 0.022168 | . 73 Hungary | 33114 | 877 | 9149 | 23088.000000 | 342.782611 | 0.026484 | . 74 Denmark | 31201 | 663 | 24706 | 5832.000000 | 1530.729332 | 0.021249 | . 75 El Salvador | 29737 | 873 | 24643 | 4221.000000 | 458.465595 | 0.029357 | . 76 Bosnia and Herzegovina | 29075 | 908 | 22614 | 5553.000000 | 886.212725 | 0.031230 | . 77 Australia | 27206 | 897 | 24939 | 1370.000000 | 516.437774 | 0.032971 | . 78 Tunisia | 24542 | 364 | 5032 | 19146.000000 | 207.655413 | 0.014832 | . 79 Korea, South | 24422 | 427 | 22463 | 1532.000000 | 47.634853 | 0.017484 | . 80 Bulgaria | 22743 | 873 | 15448 | 6422.000000 | 327.310643 | 0.038385 | . 81 Burma | 21433 | 510 | 6084 | 14839.000000 | 39.391805 | 0.023795 | . 82 Greece | 20947 | 424 | 1347 | 19176.000000 | 200.967931 | 0.020242 | . 83 Cameroon | 20924 | 420 | 19764 | 740.000000 | 78.822072 | 0.020073 | . 84 Jordan | 20200 | 131 | 5575 | 14494.000000 | 197.978269 | 0.006485 | . 85 Cote d&#39;Ivoire | 19935 | 120 | 19550 | 265.000000 | 75.573554 | 0.006020 | . 86 North Macedonia | 19413 | 772 | 15749 | 2892.000000 | 931.803128 | 0.039767 | . 87 Croatia | 18447 | 309 | 16308 | 1830.000000 | 449.349470 | 0.016751 | . 88 Madagascar | 16633 | 235 | 15808 | 590.000000 | 60.066406 | 0.014129 | . 89 Kosovo | 15938 | 638 | 14143 | 1157.000000 | 880.374466 | 0.040030 | . 90 Zambia | 15224 | 335 | 14342 | 547.000000 | 82.811338 | 0.022005 | . 91 Senegal | 15174 | 313 | 12998 | 1863.000000 | 90.623886 | 0.020627 | . 92 Norway | 15013 | 275 | 11863 | 2875.000000 | 276.929161 | 0.018317 | . 93 Albania | 14730 | 407 | 9115 | 5208.000000 | 511.849329 | 0.027631 | . 94 Slovakia | 14689 | 55 | 5200 | 9434.000000 | 269.046896 | 0.003744 | . 95 Malaysia | 13993 | 141 | 10501 | 3351.000000 | 43.233643 | 0.010076 | . 96 Sudan | 13668 | 836 | 6764 | 6068.000000 | 31.170417 | 0.061165 | . 97 Montenegro | 12794 | 190 | 8907 | 3697.000000 | 2037.060035 | 0.014851 | . 98 Namibia | 11714 | 126 | 9673 | 1915.000000 | 461.014847 | 0.010756 | . 99 Finland | 11049 | 346 | 8500 | 2203.000000 | 199.414589 | 0.031315 | . 100 Guinea | 10863 | 68 | 10176 | 619.000000 | 82.716607 | 0.006260 | . 101 Congo (Kinshasa) | 10804 | 276 | 10239 | 289.000000 | 12.063232 | 0.025546 | . 102 Maldives | 10656 | 34 | 9547 | 1075.000000 | 1971.354677 | 0.003191 | . 103 Tajikistan | 10055 | 78 | 8876 | 1101.000000 | 105.424381 | 0.007757 | . 104 Georgia | 9753 | 63 | 5235 | 4455.000000 | 244.486642 | 0.006460 | . 105 Mozambique | 9494 | 68 | 6812 | 2614.000000 | 30.375517 | 0.007162 | . 106 Uganda | 9260 | 85 | 5588 | 3587.000000 | 20.244420 | 0.009179 | . 107 Luxembourg | 9119 | 128 | 7900 | 1091.000000 | 1456.765116 | 0.014037 | . 108 Haiti | 8838 | 229 | 7013 | 1596.000000 | 77.509094 | 0.025911 | . 109 Gabon | 8815 | 54 | 8164 | 597.000000 | 396.050191 | 0.006126 | . 110 Zimbabwe | 7919 | 229 | 6441 | 1249.000000 | 53.280219 | 0.028918 | . 111 Mauritania | 7535 | 162 | 7212 | 161.000000 | 162.054860 | 0.021500 | . 112 Jamaica | 7191 | 126 | 2700 | 4365.000000 | 242.843939 | 0.017522 | . 113 Slovenia | 7120 | 159 | 4535 | 2426.000000 | 342.483544 | 0.022331 | . 114 Cabo Verde | 6624 | 71 | 5684 | 869.000000 | 1191.392620 | 0.010719 | . 115 Cuba | 5898 | 123 | 5321 | 454.000000 | 52.072040 | 0.020855 | . 116 Malawi | 5803 | 180 | 4575 | 1048.000000 | 30.334624 | 0.031018 | . 117 Angola | 5725 | 211 | 2598 | 2916.000000 | 17.419075 | 0.036856 | . 118 Eswatini | 5617 | 113 | 5196 | 308.000000 | 484.155688 | 0.020118 | . 119 Lithuania | 5483 | 101 | 2600 | 2782.000000 | 201.411238 | 0.018421 | . 120 Djibouti | 5423 | 61 | 5353 | 9.000000 | 548.885529 | 0.011248 | . 121 Nicaragua | 5264 | 153 | 4225 | 886.000000 | 79.461953 | 0.029065 | . 122 Congo (Brazzaville) | 5089 | 89 | 3887 | 1113.000000 | 92.223906 | 0.017489 | . 123 Equatorial Guinea | 5052 | 83 | 4894 | 75.000000 | 360.089381 | 0.016429 | . 124 Suriname | 4979 | 106 | 4781 | 92.000000 | 848.740441 | 0.021289 | . 125 Trinidad and Tobago | 4887 | 84 | 3010 | 1793.000000 | 349.198387 | 0.017188 | . 126 Rwanda | 4883 | 29 | 3408 | 1446.000000 | 37.700133 | 0.005939 | . 127 Central African Republic | 4852 | 62 | 1914 | 2876.000000 | 100.460395 | 0.012778 | . 128 Bahamas | 4713 | 102 | 2607 | 2004.000000 | 1198.480348 | 0.021642 | . 129 Syria | 4504 | 212 | 1198 | 3094.000000 | 25.736177 | 0.047069 | . 130 Sri Lanka | 4459 | 13 | 3274 | 1172.000000 | 20.823556 | 0.002915 | . 131 Somalia | 3745 | 99 | 3010 | 636.000000 | 23.563508 | 0.026435 | . 132 Estonia | 3715 | 67 | 2813 | 835.000000 | 280.052075 | 0.018035 | . 133 Thailand | 3622 | 59 | 3439 | 124.000000 | 5.189113 | 0.016289 | . 134 Gambia | 3613 | 117 | 2235 | 1261.000000 | 149.503613 | 0.032383 | . 135 Malta | 3442 | 41 | 2865 | 536.000000 | 779.546088 | 0.011912 | . 136 Guyana | 3292 | 95 | 2084 | 1113.000000 | 418.531858 | 0.028858 | . 137 Mali | 3210 | 131 | 2502 | 577.000000 | 15.851199 | 0.040810 | . 138 Botswana | 3172 | 18 | 834 | 2320.000000 | 134.885452 | 0.005675 | . 139 Iceland | 3172 | 10 | 2366 | 796.000000 | 929.523810 | 0.003153 | . 140 South Sudan | 2748 | 50 | 1290 | 1408.000000 | 24.549460 | 0.018195 | . 141 Andorra | 2568 | 53 | 1715 | 800.000000 | 3323.626480 | 0.020639 | . 142 Benin | 2411 | 41 | 1973 | 397.000000 | 19.887492 | 0.017005 | . 143 Guinea-Bissau | 2385 | 40 | 1728 | 617.000000 | 121.189148 | 0.016771 | . 144 Belize | 2310 | 34 | 1427 | 849.000000 | 580.955231 | 0.014719 | . 145 Sierra Leone | 2287 | 72 | 1716 | 499.000000 | 28.669980 | 0.031482 | . 146 Latvia | 2261 | 40 | 1322 | 899.000000 | 119.870512 | 0.017691 | . 147 Burkina Faso | 2222 | 59 | 1478 | 685.000000 | 10.629912 | 0.026553 | . 148 Uruguay | 2206 | 49 | 1890 | 267.000000 | 63.505278 | 0.022212 | . 149 Yemen | 2049 | 593 | 1328 | 128.000000 | 6.869852 | 0.289409 | . 150 Togo | 1898 | 49 | 1419 | 430.000000 | 22.926202 | 0.025817 | . 151 Cyprus | 1897 | 24 | 1369 | 504.000000 | 157.119536 | 0.012652 | . 152 New Zealand | 1864 | 25 | 1800 | 39.000000 | 38.654291 | 0.013412 | . 153 Lesotho | 1767 | 40 | 926 | 801.000000 | 82.483293 | 0.022637 | . 154 Liberia | 1355 | 82 | 1245 | 28.000000 | 26.790956 | 0.060517 | . 155 Chad | 1251 | 89 | 1090 | 72.000000 | 7.616040 | 0.071143 | . 156 Niger | 1200 | 69 | 1122 | 9.000000 | 4.957318 | 0.057500 | . 157 Vietnam | 1099 | 35 | 1023 | 41.000000 | 1.129049 | 0.031847 | . 158 Sao Tome and Principe | 914 | 15 | 888 | 11.000000 | 417.045003 | 0.016411 | . 159 San Marino | 732 | 42 | 680 | 10.000000 | 2156.874300 | 0.057377 | . 160 Diamond Princess | 712 | 13 | 651 | 48.000000 | 0.000000 | 0.018258 | . 161 Papua New Guinea | 541 | 7 | 527 | 7.000000 | 6.046701 | 0.012939 | . 162 Taiwan* | 523 | 7 | 486 | 30.000000 | 2.195931 | 0.013384 | . 163 Burundi | 515 | 1 | 472 | 42.000000 | 4.331086 | 0.001942 | . 164 Tanzania | 509 | 21 | 183 | 305.000000 | 0.852108 | 0.041257 | . 165 Comoros | 491 | 7 | 468 | 16.000000 | 56.463066 | 0.014257 | . 166 Eritrea | 398 | 0 | 358 | 40.000000 | 11.222563 | 0.000000 | . 167 Mauritius | 395 | 10 | 358 | 27.000000 | 31.059148 | 0.025316 | . 168 Mongolia | 315 | 0 | 308 | 7.000000 | 9.608662 | 0.000000 | . 169 Bhutan | 304 | 0 | 252 | 52.000000 | 39.398039 | 0.000000 | . 170 Cambodia | 281 | 0 | 276 | 5.000000 | 1.680725 | 0.000000 | . 171 Monaco | 227 | 2 | 202 | 23.000000 | 578.432372 | 0.008811 | . 172 Barbados | 203 | 7 | 182 | 14.000000 | 70.640392 | 0.034483 | . 173 Seychelles | 148 | 0 | 143 | 5.000000 | 150.498271 | 0.000000 | . 174 Brunei | 146 | 3 | 143 | 0.000000 | 33.372725 | 0.020548 | . 175 Liechtenstein | 131 | 1 | 116 | 14.000000 | 343.498440 | 0.007634 | . 176 Antigua and Barbuda | 108 | 3 | 97 | 8.000000 | 110.285107 | 0.027778 | . 177 Saint Vincent and the Grenadines | 64 | 0 | 64 | 0.000000 | 57.685201 | 0.000000 | . 178 Fiji | 32 | 2 | 28 | 2.000000 | 3.569660 | 0.062500 | . 179 Dominica | 31 | 0 | 24 | 7.000000 | 43.060938 | 0.000000 | . 180 Timor-Leste | 28 | 0 | 28 | 0.000000 | 2.123719 | 0.000000 | . 181 Saint Lucia | 27 | 0 | 27 | 0.000000 | 14.703560 | 0.000000 | . 182 Grenada | 24 | 0 | 24 | 0.000000 | 21.329731 | 0.000000 | . 183 Laos | 23 | 0 | 22 | 1.000000 | 0.316127 | 0.000000 | . 184 Saint Kitts and Nevis | 19 | 0 | 17 | 2.000000 | 35.719657 | 0.000000 | . 185 Holy See | 12 | 0 | 12 | 0.000000 | 1483.312732 | 0.000000 | . 186 Western Sahara | 10 | 1 | 8 | 1.000000 | 1.674116 | 0.100000 | . 187 MS Zaandam | 9 | 2 | 0 | 7.000000 | 0.000000 | 0.222222 | . unique_provinces = list(latest_data[&#39;Province_State&#39;].unique()) . Getting the latest information about provinces/states that have confirmed coronavirus cases . province_confirmed_cases = [] province_country = [] province_death_cases = [] # province_recovery_cases = [] province_active = [] province_incidence_rate = [] province_mortality_rate = [] no_cases = [] for i in unique_provinces: cases = latest_data[latest_data[&#39;Province_State&#39;]==i][&#39;Confirmed&#39;].sum() if cases &gt; 0: province_confirmed_cases.append(cases) else: no_cases.append(i) # remove areas with no confirmed cases for i in no_cases: unique_provinces.remove(i) unique_provinces = [k for k, v in sorted(zip(unique_provinces, province_confirmed_cases), key=operator.itemgetter(1), reverse=True)] for i in range(len(unique_provinces)): province_confirmed_cases[i] = latest_data[latest_data[&#39;Province_State&#39;]==unique_provinces[i]][&#39;Confirmed&#39;].sum() province_country.append(latest_data[latest_data[&#39;Province_State&#39;]==unique_provinces[i]][&#39;Country_Region&#39;].unique()[0]) province_death_cases.append(latest_data[latest_data[&#39;Province_State&#39;]==unique_provinces[i]][&#39;Deaths&#39;].sum()) # province_recovery_cases.append(latest_data[latest_data[&#39;Province_State&#39;]==unique_provinces[i]][&#39;Recovered&#39;].sum()) province_active.append(latest_data[latest_data[&#39;Province_State&#39;]==unique_provinces[i]][&#39;Active&#39;].sum()) province_incidence_rate.append(latest_data[latest_data[&#39;Province_State&#39;]==unique_provinces[i]][&#39;Incidence_Rate&#39;].sum()) province_mortality_rate.append(province_death_cases[i]/province_confirmed_cases[i]) . # # handle nan if there is any, it is usually a float: float(&#39;nan&#39;) # for i in range(len(unique_provinces)): # if type(unique_provinces[i]) == float: # nan_indices.append(i) # unique_provinces = list(unique_provinces) # province_confirmed_cases = list(province_confirmed_cases) # for i in nan_indices: # unique_provinces.pop(i) # province_confirmed_cases.pop(i) . province_limit = 100 province_df = pd.DataFrame({&#39;Province/State Name&#39;: unique_provinces[:province_limit], &#39;Country&#39;: province_country[:province_limit], &#39;Number of Confirmed Cases&#39;: province_confirmed_cases[:province_limit], &#39;Number of Deaths&#39;: province_death_cases[:province_limit],&#39;Number of Active Cases&#39; : province_active[:province_limit], &#39;Incidence Rate&#39; : province_incidence_rate[:province_limit], &#39;Mortality Rate&#39;: province_mortality_rate[:province_limit]}) # number of cases per country/region province_df.style.background_gradient(cmap=&#39;Oranges&#39;) . Province/State Name Country Number of Confirmed Cases Number of Deaths Number of Active Cases Incidence Rate Mortality Rate . 0 Maharashtra | India | 1480489 | 39072 | 244976.000000 | 1202.239913 | 0.026391 | . 1 Sao Paulo | Brazil | 1016755 | 36669 | 85082.000000 | 2214.233574 | 0.036065 | . 2 California | US | 841928 | 16338 | 825590.000000 | 97042.918220 | 0.019405 | . 3 Texas | US | 803690 | 16661 | 787029.000000 | 585661.650126 | 0.020731 | . 4 Andhra Pradesh | India | 734427 | 6086 | 49513.000000 | 1362.487515 | 0.008287 | . 5 Florida | US | 722707 | 14904 | 707803.000000 | 252486.050695 | 0.020622 | . 6 Karnataka | India | 668652 | 9574 | 116172.000000 | 989.676461 | 0.014318 | . 7 Tamil Nadu | India | 635855 | 9984 | 45135.000000 | 816.861062 | 0.015702 | . 8 New York | US | 468268 | 33226 | 435042.000000 | 68518.583673 | 0.070955 | . 9 England | United Kingdom | 465704 | 37753 | 427951.000000 | 831.953009 | 0.081067 | . 10 Uttar Pradesh | India | 424326 | 6200 | 43154.000000 | 178.376131 | 0.014611 | . 11 Lima | Peru | 379347 | 14804 | 364543.000000 | 3569.148986 | 0.039025 | . 12 Georgia | US | 326142 | 7259 | 318883.000000 | 520180.869873 | 0.022257 | . 13 Bahia | Brazil | 319981 | 7021 | 6595.000000 | 2151.412782 | 0.021942 | . 14 Moscow | Russia | 314788 | 5442 | 52984.000000 | 2517.001603 | 0.017288 | . 15 Minas Gerais | Brazil | 313032 | 7811 | 26949.000000 | 1478.742929 | 0.024953 | . 16 Illinois | US | 310335 | 9127 | 301208.000000 | 183939.380296 | 0.029410 | . 17 Delhi | India | 298107 | 5616 | 22186.000000 | 1593.224535 | 0.018839 | . 18 Metropolitana | Chile | 289158 | 9404 | 3263.000000 | 4065.314289 | 0.032522 | . 19 Capital District | Colombia | 281534 | 6982 | 24365.000000 | 3798.063990 | 0.024800 | . 20 West Bengal | India | 280504 | 5376 | 28361.000000 | 281.604219 | 0.019166 | . 21 Rio de Janeiro | Brazil | 277439 | 18969 | 5245.000000 | 1606.949991 | 0.068372 | . 22 Ceara | Brazil | 256764 | 9094 | 26437.000000 | 2811.671122 | 0.035418 | . 23 Madrid | Spain | 255615 | 9634 | 205245.000000 | 3848.667703 | 0.037689 | . 24 Kerala | India | 253405 | 906 | 92246.000000 | 709.829002 | 0.003575 | . 25 Odisha | India | 240998 | 958 | 26368.000000 | 519.881490 | 0.003975 | . 26 Para | Brazil | 235948 | 6623 | 8603.000000 | 2742.667704 | 0.028070 | . 27 Goias | Brazil | 224181 | 5020 | 5460.000000 | 3194.210494 | 0.022393 | . 28 North Carolina | US | 222969 | 3693 | 219276.000000 | 214244.682876 | 0.016563 | . 29 Santa Catarina | Brazil | 222652 | 2880 | 8517.000000 | 3107.586714 | 0.012935 | . 30 Arizona | US | 222538 | 5733 | 216806.000000 | 46542.262687 | 0.025762 | . 31 Punjab | India | 221132 | 5959 | 13082.000000 | 492.123143 | 0.026948 | . 32 New Jersey | US | 209850 | 16152 | 193698.000000 | 44057.873952 | 0.076969 | . 33 Rio Grande do Sul | Brazil | 207706 | 5035 | 9134.000000 | 1825.627466 | 0.024241 | . 34 Tennessee | US | 207455 | 2642 | 204813.000000 | 307984.113874 | 0.012735 | . 35 Telangana | India | 206644 | 1201 | 26368.000000 | 524.973724 | 0.005812 | . 36 Distrito Federal | Brazil | 197369 | 3378 | 7613.000000 | 6545.653653 | 0.017115 | . 37 Bihar | India | 191985 | 927 | 11326.000000 | 153.834226 | 0.004829 | . 38 Assam | India | 190209 | 785 | 31786.000000 | 534.189321 | 0.004127 | . 39 Parana | Brazil | 187729 | 4703 | 46125.000000 | 1641.855046 | 0.025052 | . 40 Maranhao | Brazil | 176995 | 3828 | 6041.000000 | 2501.632114 | 0.021628 | . 41 Pennsylvania | US | 171527 | 8247 | 163280.000000 | 58419.326863 | 0.048080 | . 42 Louisiana | US | 170097 | 5604 | 164493.000000 | 254401.938988 | 0.032946 | . 43 Ohio | US | 162723 | 4970 | 157753.000000 | 101558.871904 | 0.030543 | . 44 Amazonas | Brazil | 162616 | 4534 | 35454.000000 | 10676.880605 | 0.027882 | . 45 Alabama | US | 161418 | 2601 | 158817.000000 | 240283.508340 | 0.016113 | . 46 South Carolina | US | 153705 | 3502 | 150203.000000 | 145465.750509 | 0.022784 | . 47 Virginia | US | 153451 | 3300 | 150151.000000 | 234650.892269 | 0.021505 | . 48 Pernambuco | Brazil | 151139 | 8379 | 11625.000000 | 1581.436405 | 0.055439 | . 49 Rajasthan | India | 150467 | 1590 | 21351.000000 | 185.686791 | 0.010567 | . 50 Catalonia | Spain | 150438 | 5861 | 118374.000000 | 1988.229325 | 0.038960 | . 51 Gujarat | India | 146673 | 3531 | 16485.000000 | 229.634400 | 0.024074 | . 52 Michigan | US | 145092 | 7169 | 137923.000000 | 73053.547245 | 0.049410 | . 53 Madhya Pradesh | India | 140307 | 2518 | 17522.000000 | 164.372893 | 0.017946 | . 54 Sindh | Pakistan | 139195 | 2535 | 4477.000000 | 290.679639 | 0.018212 | . 55 Wisconsin | US | 138698 | 1415 | 137283.000000 | 142083.779635 | 0.010202 | . 56 Missouri | US | 137420 | 2238 | 135183.000000 | 217413.591916 | 0.016286 | . 57 Haryana | India | 137398 | 1528 | 11029.000000 | 487.145898 | 0.011121 | . 58 Espirito Santo | Brazil | 136590 | 3617 | 7170.000000 | 3398.902617 | 0.026481 | . 59 Massachusetts | US | 136492 | 9557 | 126935.000000 | 19991.555711 | 0.070019 | . 60 Ciudad de Mexico | Mexico | 136154 | 13779 | 7226.000000 | 1509.694638 | 0.101202 | . 61 Chhattisgarh | India | 131739 | 1134 | 26777.000000 | 447.540312 | 0.008608 | . 62 Mato Grosso | Brazil | 129038 | 3525 | 15023.000000 | 3703.236020 | 0.027318 | . 63 Maryland | US | 128664 | 3973 | 124691.000000 | 39357.659455 | 0.030879 | . 64 Indiana | US | 128227 | 3727 | 124500.000000 | 145061.251419 | 0.029066 | . 65 Paraiba | Brazil | 124315 | 2884 | 22232.000000 | 3093.854425 | 0.023199 | . 66 Antioquia | Colombia | 122871 | 2568 | 10894.000000 | 1917.731293 | 0.020900 | . 67 Lombardia | Italy | 109186 | 16978 | 9877.000000 | 1085.285989 | 0.155496 | . 68 Minnesota | US | 106651 | 2154 | 104497.000000 | 142671.198402 | 0.020197 | . 69 Mississippi | US | 102241 | 3051 | 99190.000000 | 323480.703516 | 0.029841 | . 70 Piaui | Brazil | 99960 | 2180 | -74.000000 | 3053.867025 | 0.021809 | . 71 Iowa | US | 95093 | 1419 | 93674.000000 | 263925.786771 | 0.014922 | . 72 Oklahoma | US | 94352 | 1075 | 93277.000000 | 164942.077198 | 0.011394 | . 73 Washington | US | 91208 | 2177 | 89031.000000 | 49287.775914 | 0.023869 | . 74 Jharkhand | India | 89702 | 767 | 9759.000000 | 232.425042 | 0.008551 | . 75 Arkansas | US | 88880 | 1482 | 87398.000000 | 226105.098240 | 0.016674 | . 76 Alagoas | Brazil | 88426 | 2115 | 1053.000000 | 2649.581690 | 0.023918 | . 77 Mexico | Mexico | 87230 | 9965 | 3359.000000 | 500.522441 | 0.114238 | . 78 Nevada | US | 83347 | 1636 | 81711.000000 | 16174.700927 | 0.019629 | . 79 Quebec | Canada | 81914 | 5906 | 8273.000000 | 959.441647 | 0.072100 | . 80 Jammu and Kashmir | India | 81097 | 1282 | 12131.000000 | 596.024494 | 0.015808 | . 81 Utah | US | 80446 | 496 | 79950.000000 | 26861.823824 | 0.006166 | . 82 Sergipe | Brazil | 78692 | 2072 | 4459.000000 | 3423.332185 | 0.026331 | . 83 Kentucky | US | 76587 | 1223 | 75364.000000 | 167179.116739 | 0.015969 | . 84 Moscow Oblast | Russia | 75712 | 1384 | 16752.000000 | 1009.037921 | 0.018280 | . 85 Nordrhein-Westfalen | Germany | 75671 | 1894 | 8626.000000 | 421.973304 | 0.025029 | . 86 Colorado | US | 74899 | 2085 | 72814.000000 | 63314.256120 | 0.027837 | . 87 Mato Grosso do Sul | Brazil | 73027 | 1385 | 5027.000000 | 2627.828999 | 0.018966 | . 88 Rio Grande do Norte | Brazil | 71898 | 2412 | 28037.000000 | 2050.214252 | 0.033548 | . 89 Bayern | Germany | 71034 | 2681 | 5078.000000 | 543.209571 | 0.037742 | . 90 Andalusia | Spain | 70560 | 1941 | 57948.000000 | 837.268412 | 0.027509 | . 91 Tocantins | Brazil | 69969 | 985 | 14187.000000 | 4448.503560 | 0.014078 | . 92 Atlantico | Colombia | 68465 | 3080 | 1370.000000 | 2700.238255 | 0.044986 | . 93 Hubei | China | 68139 | 4512 | 0.000000 | 115.158019 | 0.066218 | . 94 Rondonia | Brazil | 67181 | 1385 | 7013.000000 | 3780.106627 | 0.020616 | . 95 Valle del Cauca | Colombia | 66796 | 2395 | 6447.000000 | 1492.352576 | 0.035855 | . 96 Kansas | US | 62941 | 713 | 62228.000000 | 197169.529217 | 0.011328 | . 97 Connecticut | US | 59364 | 4522 | 54842.000000 | 10522.703133 | 0.076174 | . 98 Ontario | Canada | 58202 | 3039 | 5431.000000 | 395.613679 | 0.052215 | . 99 Uttarakhand | India | 52959 | 688 | 8367.000000 | 470.710767 | 0.012991 | . us_states = list(latest_data[latest_data[&#39;Country_Region&#39;]==&#39;US&#39;][&#39;Province_State&#39;].unique()) . state_confirmed_cases = [] state_death_cases = [] # state_recovery_cases = [] state_active = [] state_incidence_rate = [] state_mortality_rate = [] no_cases = [] for i in us_states: cases = latest_data[latest_data[&#39;Province_State&#39;]==i][&#39;Confirmed&#39;].sum() if cases &gt; 0: state_confirmed_cases.append(cases) else: no_cases.append(i) # remove areas with no confirmed cases for i in no_cases: us_states.remove(i) us_states = [k for k, v in sorted(zip(us_states, state_confirmed_cases), key=operator.itemgetter(1), reverse=True)] for i in range(len(us_states)): state_confirmed_cases[i] = latest_data[latest_data[&#39;Province_State&#39;]==us_states[i]][&#39;Confirmed&#39;].sum() state_death_cases.append(latest_data[latest_data[&#39;Province_State&#39;]==us_states[i]][&#39;Deaths&#39;].sum()) # state_recovery_cases.append(latest_data[latest_data[&#39;Province_State&#39;]==us_states[i]][&#39;Recovered&#39;].sum()) state_active.append(latest_data[latest_data[&#39;Province_State&#39;]==us_states[i]][&#39;Active&#39;].sum()) state_incidence_rate.append(latest_data[latest_data[&#39;Province_State&#39;]==us_states[i]][&#39;Incidence_Rate&#39;].sum()) state_mortality_rate.append(state_death_cases[i]/state_confirmed_cases[i]) . state_df = pd.DataFrame({&#39;State Name&#39;: us_states, &#39;Number of Confirmed Cases&#39;: state_confirmed_cases, &#39;Number of Deaths&#39;: state_death_cases, &#39;Number of Active Cases&#39; : state_active, &#39;Incidence Rate&#39; : state_incidence_rate, &#39;Mortality Rate&#39;: state_mortality_rate}) # number of cases per country/region state_df.style.background_gradient(cmap=&#39;Oranges&#39;) . State Name Number of Confirmed Cases Number of Deaths Number of Active Cases Incidence Rate Mortality Rate . 0 California | 841928 | 16338 | 825590.000000 | 97042.918220 | 0.019405 | . 1 Texas | 803690 | 16661 | 787029.000000 | 585661.650126 | 0.020731 | . 2 Florida | 722707 | 14904 | 707803.000000 | 252486.050695 | 0.020622 | . 3 New York | 468268 | 33226 | 435042.000000 | 68518.583673 | 0.070955 | . 4 Georgia | 326142 | 7259 | 318883.000000 | 520180.869873 | 0.022257 | . 5 Illinois | 310335 | 9127 | 301208.000000 | 183939.380296 | 0.029410 | . 6 North Carolina | 222969 | 3693 | 219276.000000 | 214244.682876 | 0.016563 | . 7 Arizona | 222538 | 5733 | 216806.000000 | 46542.262687 | 0.025762 | . 8 New Jersey | 209850 | 16152 | 193698.000000 | 44057.873952 | 0.076969 | . 9 Tennessee | 207455 | 2642 | 204813.000000 | 307984.113874 | 0.012735 | . 10 Pennsylvania | 171527 | 8247 | 163280.000000 | 58419.326863 | 0.048080 | . 11 Louisiana | 170097 | 5604 | 164493.000000 | 254401.938988 | 0.032946 | . 12 Ohio | 162723 | 4970 | 157753.000000 | 101558.871904 | 0.030543 | . 13 Alabama | 161418 | 2601 | 158817.000000 | 240283.508340 | 0.016113 | . 14 South Carolina | 153705 | 3502 | 150203.000000 | 145465.750509 | 0.022784 | . 15 Virginia | 153451 | 3300 | 150151.000000 | 234650.892269 | 0.021505 | . 16 Michigan | 145092 | 7169 | 137923.000000 | 73053.547245 | 0.049410 | . 17 Wisconsin | 138698 | 1415 | 137283.000000 | 142083.779635 | 0.010202 | . 18 Missouri | 137420 | 2238 | 135183.000000 | 217413.591916 | 0.016286 | . 19 Massachusetts | 136492 | 9557 | 126935.000000 | 19991.555711 | 0.070019 | . 20 Maryland | 128664 | 3973 | 124691.000000 | 39357.659455 | 0.030879 | . 21 Indiana | 128227 | 3727 | 124500.000000 | 145061.251419 | 0.029066 | . 22 Minnesota | 106651 | 2154 | 104497.000000 | 142671.198402 | 0.020197 | . 23 Mississippi | 102241 | 3051 | 99190.000000 | 323480.703516 | 0.029841 | . 24 Iowa | 95093 | 1419 | 93674.000000 | 263925.786771 | 0.014922 | . 25 Oklahoma | 94352 | 1075 | 93277.000000 | 164942.077198 | 0.011394 | . 26 Washington | 91208 | 2177 | 89031.000000 | 49287.775914 | 0.023869 | . 27 Arkansas | 88880 | 1482 | 87398.000000 | 226105.098240 | 0.016674 | . 28 Nevada | 83347 | 1636 | 81711.000000 | 16174.700927 | 0.019629 | . 29 Utah | 80446 | 496 | 79950.000000 | 26861.823824 | 0.006166 | . 30 Kentucky | 76587 | 1223 | 75364.000000 | 167179.116739 | 0.015969 | . 31 Colorado | 74899 | 2085 | 72814.000000 | 63314.256120 | 0.027837 | . 32 Kansas | 62941 | 713 | 62228.000000 | 197169.529217 | 0.011328 | . 33 Connecticut | 59364 | 4522 | 54842.000000 | 10522.703133 | 0.076174 | . 34 Puerto Rico | 51768 | 705 | 51063.000000 | 94248.412403 | 0.013618 | . 35 Nebraska | 49396 | 507 | 48889.000000 | 157705.186149 | 0.010264 | . 36 Idaho | 45753 | 500 | 45253.000000 | 94813.744044 | 0.010928 | . 37 Oregon | 35634 | 583 | 35051.000000 | 35097.561461 | 0.016361 | . 38 New Mexico | 31372 | 896 | 30476.000000 | 39426.418792 | 0.028560 | . 39 South Dakota | 25906 | 258 | 25648.000000 | 174517.634859 | 0.009959 | . 40 Rhode Island | 25776 | 1126 | 24650.000000 | 6613.000880 | 0.043684 | . 41 North Dakota | 24857 | 304 | 24553.000000 | 144997.608182 | 0.012230 | . 42 Delaware | 21550 | 649 | 20901.000000 | 6781.963903 | 0.030116 | . 43 West Virginia | 17150 | 375 | 16775.000000 | 41123.215805 | 0.021866 | . 44 Montana | 16063 | 193 | 15870.000000 | 84543.077213 | 0.012015 | . 45 District of Columbia | 15652 | 631 | 15021.000000 | 2217.785643 | 0.040314 | . 46 Hawaii | 13045 | 163 | 12882.000000 | 1941.692261 | 0.012495 | . 47 Alaska | 8878 | 59 | 8819.000000 | 23052.109194 | 0.006646 | . 48 New Hampshire | 8731 | 446 | 8285.000000 | 3794.887990 | 0.051082 | . 49 Wyoming | 6899 | 53 | 6846.000000 | 26492.209596 | 0.007682 | . 50 Maine | 5603 | 142 | 5461.000000 | 4121.804077 | 0.025344 | . 51 Guam | 2868 | 57 | 2811.000000 | 1746.341998 | 0.019874 | . 52 Vermont | 1827 | 58 | 1769.000000 | 3107.296719 | 0.031746 | . 53 Virgin Islands | 1322 | 20 | 1302.000000 | 1232.427192 | 0.015129 | . 54 Grand Princess | 116 | 3 | 100.000000 | 0.000000 | 0.025862 | . 55 Northern Mariana Islands | 75 | 2 | 73.000000 | 136.007544 | 0.026667 | . 56 Diamond Princess | 49 | 1 | 49.000000 | 0.000000 | 0.020408 | . Bar Chart Visualizations for COVID-19 . us_confirmed = latest_data[latest_data[&#39;Country_Region&#39;]==&#39;US&#39;][&#39;Confirmed&#39;].sum() outside_us_confirmed = np.sum(country_confirmed_cases) - us_confirmed plt.figure(figsize=(16, 9)) plt.barh(&#39;United States&#39;, us_confirmed) plt.barh(&#39;Outside United States&#39;, outside_us_confirmed) plt.title(&#39;# of Coronavirus Confirmed Cases&#39;, size=20) plt.xticks(size=20) plt.yticks(size=20) plt.show() . print(&#39;Outside United States {} cases:&#39;.format(outside_us_confirmed)) print(&#39;United States {} cases&#39;.format(us_confirmed)) print(&#39;Total: {} cases&#39;.format(us_confirmed+outside_us_confirmed)) . Outside United States 28606544 cases: United States 7549682 cases Total: 36156226 cases . Only show 15 countries with the most confirmed cases, the rest are grouped into the other category . visual_unique_countries = [] visual_confirmed_cases = [] others = np.sum(country_confirmed_cases[10:]) for i in range(len(country_confirmed_cases[:10])): visual_unique_countries.append(unique_countries[i]) visual_confirmed_cases.append(country_confirmed_cases[i]) visual_unique_countries.append(&#39;Others&#39;) visual_confirmed_cases.append(others) . Visual Representations (bar charts and pie charts) . def plot_bar_graphs(x, y, title): plt.figure(figsize=(16, 12)) plt.barh(x, y) plt.title(title, size=20) plt.xticks(size=20) plt.yticks(size=20) plt.show() . def plot_bar_graphs_tall(x, y, title): plt.figure(figsize=(19, 18)) plt.barh(x, y) plt.title(title, size=25) plt.xticks(size=25) plt.yticks(size=25) plt.show() . plot_bar_graphs(visual_unique_countries, visual_confirmed_cases, &#39;# of Covid-19 Confirmed Cases in Countries/Regions&#39;) . log_country_confirmed_cases = [math.log10(i) for i in visual_confirmed_cases] plot_bar_graphs(visual_unique_countries, log_country_confirmed_cases, &#39;Common Log # of Coronavirus Confirmed Cases in Countries/Regions&#39;) . Only show 10 provinces with the most confirmed cases, the rest are grouped into the other category . visual_unique_provinces = [] visual_confirmed_cases2 = [] others = np.sum(province_confirmed_cases[10:]) for i in range(len(province_confirmed_cases[:10])): visual_unique_provinces.append(unique_provinces[i]) visual_confirmed_cases2.append(province_confirmed_cases[i]) visual_unique_provinces.append(&#39;Others&#39;) visual_confirmed_cases2.append(others) . plot_bar_graphs(visual_unique_provinces, visual_confirmed_cases2, &#39;# of Coronavirus Confirmed Cases in Provinces/States&#39;) . log_province_confirmed_cases = [math.log10(i) for i in visual_confirmed_cases2] plot_bar_graphs(visual_unique_provinces, log_province_confirmed_cases, &#39;Log of # of Coronavirus Confirmed Cases in Provinces/States&#39;) . Pie Chart Visualizations for COVID-19 . def plot_pie_charts(x, y, title): # more muted color c = [&#39;lightcoral&#39;, &#39;rosybrown&#39;, &#39;sandybrown&#39;, &#39;navajowhite&#39;, &#39;gold&#39;, &#39;khaki&#39;, &#39;lightskyblue&#39;, &#39;turquoise&#39;, &#39;lightslategrey&#39;, &#39;thistle&#39;, &#39;pink&#39;] plt.figure(figsize=(20,15)) plt.title(title, size=20) plt.pie(y, colors=c,shadow=True, labels=y) plt.legend(x, loc=&#39;best&#39;, fontsize=12) plt.show() . plot_pie_charts(visual_unique_countries, visual_confirmed_cases, &#39;Covid-19 Confirmed Cases per Country&#39;) . plot_pie_charts(visual_unique_provinces, visual_confirmed_cases2, &#39;Covid-19 Confirmed Cases per State/Province/Region&#39;) . Plotting countries with regional data using a pie chart . def plot_pie_country_with_regions(country_name, title): regions = list(latest_data[latest_data[&#39;Country_Region&#39;]==country_name][&#39;Province_State&#39;].unique()) confirmed_cases = [] no_cases = [] for i in regions: cases = latest_data[latest_data[&#39;Province_State&#39;]==i][&#39;Confirmed&#39;].sum() if cases &gt; 0: confirmed_cases.append(cases) else: no_cases.append(i) # remove areas with no confirmed cases for i in no_cases: regions.remove(i) # only show the top 5 states regions = [k for k, v in sorted(zip(regions, confirmed_cases), key=operator.itemgetter(1), reverse=True)] for i in range(len(regions)): confirmed_cases[i] = latest_data[latest_data[&#39;Province_State&#39;]==regions[i]][&#39;Confirmed&#39;].sum() # additional province/state will be considered &quot;others&quot; if(len(regions)&gt;5): regions_5 = regions[:5] regions_5.append(&#39;Others&#39;) confirmed_cases_5 = confirmed_cases[:5] confirmed_cases_5.append(np.sum(confirmed_cases[5:])) plot_pie_charts(regions_5,confirmed_cases_5, title) else: plot_pie_charts(regions,confirmed_cases, title) . pie_chart_countries = [&#39;US&#39;, &#39;Brazil&#39;, &#39;Russia&#39;, &#39;India&#39;, &#39;Peru&#39;, &#39;Mexico&#39;, &#39;Canada&#39;, &#39;Australia&#39;, &#39;China&#39;, &#39;Italy&#39;, &#39;Germany&#39;, &#39;France&#39;, &#39;United Kingdom&#39;, &#39;Chile&#39;] for i in pie_chart_countries: plot_pie_country_with_regions(i, &#39;Covid-19 Confirmed Cases in {}&#39;.format(i)) . US Medical Data on Testing . us_medical_data.fillna(value=0, inplace=True) def plot_us_medical_data(): states = us_medical_data[&#39;Province_State&#39;].unique() testing_number = [] testing_rate = [] for i in states: testing_number.append(us_medical_data[us_medical_data[&#39;Province_State&#39;]==i][&#39;People_Tested&#39;].sum()) testing_rate.append(us_medical_data[us_medical_data[&#39;Province_State&#39;]==i][&#39;Testing_Rate&#39;].max()) # only show the top 15 states testing_states = [k for k, v in sorted(zip(states, testing_number), key=operator.itemgetter(1), reverse=True)] testing_rate_states = [k for k, v in sorted(zip(states, testing_rate), key=operator.itemgetter(1), reverse=True)] for i in range(len(states)): testing_number[i] = us_medical_data[us_medical_data[&#39;Province_State&#39;]==testing_states[i]][&#39;People_Tested&#39;].sum() testing_rate[i] = us_medical_data[us_medical_data[&#39;Province_State&#39;]==testing_rate_states[i]][&#39;Testing_Rate&#39;].sum() top_limit = 30 plot_bar_graphs_tall(testing_states[:top_limit], testing_number[:top_limit], &#39;Total Testing per State (Top 30)&#39;) plot_bar_graphs_tall(testing_rate_states[:top_limit], testing_rate[:top_limit], &#39;Testing Rate per 100,000 People (Top 30)&#39;) plot_us_medical_data() . Taking a look at Apple&#39;s mobility data. It can help us understand hotspot states in the US (states and territories). . def get_mobility_by_state(transport_type, state, day): return apple_mobility[apple_mobility[&#39;sub-region&#39;]==state][apple_mobility[&#39;transportation_type&#39;]==transport_type].sum()[day] . apple_mobility.head() . geo_type region transportation_type alternative_name sub-region country 2020-01-13 2020-01-14 2020-01-15 2020-01-16 ... 2020-09-28 2020-09-29 2020-09-30 2020-10-01 2020-10-02 2020-10-03 2020-10-04 2020-10-05 2020-10-06 2020-10-07 . 0 country/region | Albania | driving | NaN | NaN | NaN | 100.0 | 95.30 | 101.43 | 97.20 | ... | 117.30 | 116.38 | 119.24 | 118.79 | 130.25 | 148.03 | 136.67 | 123.11 | 117.50 | 119.25 | . 1 country/region | Albania | walking | NaN | NaN | NaN | 100.0 | 100.68 | 98.93 | 98.46 | ... | 144.95 | 160.09 | 159.83 | 154.84 | 159.32 | 166.40 | 130.23 | 168.27 | 140.04 | 154.46 | . 2 country/region | Argentina | driving | NaN | NaN | NaN | 100.0 | 97.07 | 102.45 | 111.21 | ... | 54.62 | 59.16 | 60.03 | 61.63 | 74.42 | 71.69 | 38.69 | 55.99 | 59.95 | 62.76 | . 3 country/region | Argentina | walking | NaN | NaN | NaN | 100.0 | 95.11 | 101.37 | 112.67 | ... | 48.11 | 51.81 | 50.20 | 50.76 | 57.44 | 51.42 | 29.54 | 46.80 | 49.56 | 53.87 | . 4 country/region | Australia | driving | AU | NaN | NaN | 100.0 | 102.98 | 104.21 | 108.63 | ... | 97.69 | 100.19 | 104.14 | 115.34 | 109.16 | 92.42 | 93.21 | 96.03 | 97.06 | 97.95 | . 5 rows × 275 columns . sample testing . get_mobility_by_state(&#39;walking&#39;, &#39;Connecticut&#39;, &#39;2020-07-30&#39;) . 1104.74 . revised_dates = [] for i in range(len(dates)): revised_dates.append(datetime.datetime.strptime(dates[i], &#39;%m/%d/%y&#39;).strftime(&#39;%Y-%m-%d&#39;)) . def weekday_or_weekend(date): date_obj = datetime.datetime.strptime(date, &#39;%Y-%m-%d&#39;) day_of_the_week = date_obj.weekday() if (day_of_the_week+1) % 6 == 0 or (day_of_the_week+1) % 7 == 0: return True else: return False . revised_day_since_1_22 = [i for i in range(len(revised_dates))] . import matplotlib.dates as mdates states = [&#39;New York&#39;, &#39;Connecticut&#39;, &#39;Florida&#39;, &#39;California&#39;, &#39;Texas&#39;, &#39;Georgia&#39;, &#39;Arizona&#39;, &#39;Illinois&#39;, &#39;Louisiana&#39;, &#39;Ohio&#39;, &#39;Tennessee&#39;, &#39;North Carolina&#39;, &#39;South Carolina&#39;, &#39;Alabama&#39;, &#39;Missouri&#39;, &#39;Kansas&#39;, &#39;Pennsylvania&#39;, &#39;Wisconsin&#39;, &#39;Virginia&#39;, &#39;Massachusetts&#39;, &#39;Utah&#39;, &#39;Minnesota&#39;, &#39;Oklahoma&#39;, &#39;Iowa&#39;, &#39;Arkansas&#39;, &#39;Kentucky&#39;, &#39;Puerto Rico&#39;, &#39;Colorado&#39;, &#39;New Jersey&#39;, &#39;Idaho&#39;, &#39;New Jersey&#39;, &#39;Nevada&#39;, &#39;Maryland&#39;] states.sort() # making sure the dates are in sync mobility_latest_date = apple_mobility.columns[-1] mobility_latest_index = revised_dates.index(mobility_latest_date) for state in states: # weekend and weekday mobility are separated weekday_mobility = [] weekday_mobility_dates = [] weekend_mobility = [] weekend_mobility_dates = [] for i in range(len(revised_dates)): if i &lt;= mobility_latest_index: if weekday_or_weekend(revised_dates[i]): weekend_mobility.append(get_mobility_by_state(&#39;walking&#39;, state, revised_dates[i])) weekend_mobility_dates.append(i) else: weekday_mobility.append(get_mobility_by_state(&#39;walking&#39;, state, revised_dates[i])) weekday_mobility_dates.append(i) else: pass # remove null values (they are counted as 0) for i in range(len(weekend_mobility)): if weekend_mobility[i] == 0 and i != 0: weekend_mobility[i] = weekend_mobility[i-1] elif weekend_mobility[i] == 0 and i == 0: weekend_mobility[i] = weekend_mobility[i+1] else: pass for i in range(len(weekday_mobility)): if weekday_mobility[i] == 0 and i != 0: weekday_mobility[i] = weekday_mobility[i-1] elif weekday_mobility[i] == 0 and i == 0: weekday_mobility[i] = weekday_mobility[i+1] else: pass weekday_mobility_average = moving_average(weekday_mobility, 7) weekend_mobility_average = moving_average(weekend_mobility, 7) plt.figure(figsize=(16, 10)) plt.bar(weekday_mobility_dates, weekday_mobility, color=&#39;cornflowerblue&#39;) plt.plot(weekday_mobility_dates, weekday_mobility_average, color=&#39;green&#39;) plt.bar(weekend_mobility_dates, weekend_mobility, color=&#39;salmon&#39;) plt.plot(weekend_mobility_dates, weekend_mobility_average, color=&#39;black&#39;) plt.legend([&#39;Moving average (7 days) weekday mobility&#39;, &#39;Moving Average (7 days) weekend mobility&#39;, &#39;Weekday mobility&#39;, &#39;Weekend mobility&#39;], prop={&#39;size&#39;: 25}) plt.title(&#39;{} Walking Mobility Data&#39;.format(state), size=25) plt.xlabel(&#39;Days since 1/22&#39;, size=25) plt.ylabel(&#39;Mobility Value&#39;, size=25) plt.xticks(size=25) plt.yticks(size=25) plt.show() .",
            "url": "https://siddiquisuhail.github.io/delhi_aqi/2021/06/14/My-First-Posts.html",
            "relUrl": "/2021/06/14/My-First-Posts.html",
            "date": " • Jun 14, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Importing Relevant Libraries",
            "content": "&gt; &quot;This is the blog relating to the survivours and casualties in the infamous titanic incedent.&quot; - toc:true- branch: master - badges: true - comments: true - author: Suhail Shamim - categories: [fastpages, jupyter] . # data analysis and wrangling import pandas as pd import numpy as np import random as rnd # visualization import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline # machine learning from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC, LinearSVC from sklearn.ensemble import RandomForestClassifier from sklearn.neighbors import KNeighborsClassifier from sklearn.naive_bayes import GaussianNB from sklearn.linear_model import Perceptron from sklearn.linear_model import SGDClassifier from sklearn.tree import DecisionTreeClassifier . . Acquiring Traning and Test data . train_df = pd.read_csv(&#39;train.csv&#39;) test_df = pd.read_csv(&#39;test.csv&#39;) . We combine the Train and Test to run certaion operations on certain datasets together . combine = [train_df, test_df] . Analyse by describing data. . print(train_df.columns.values) . [&#39;PassengerId&#39; &#39;Survived&#39; &#39;Pclass&#39; &#39;Name&#39; &#39;Sex&#39; &#39;Age&#39; &#39;SibSp&#39; &#39;Parch&#39; &#39;Ticket&#39; &#39;Fare&#39; &#39;Cabin&#39; &#39;Embarked&#39;] . # preview the data train_df.head() . . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | . train_df.tail() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 886 887 | 0 | 2 | Montvila, Rev. Juozas | male | 27.0 | 0 | 0 | 211536 | 13.00 | NaN | S | . 887 888 | 1 | 1 | Graham, Miss. Margaret Edith | female | 19.0 | 0 | 0 | 112053 | 30.00 | B42 | S | . 888 889 | 0 | 3 | Johnston, Miss. Catherine Helen &quot;Carrie&quot; | female | NaN | 1 | 2 | W./C. 6607 | 23.45 | NaN | S | . 889 890 | 1 | 1 | Behr, Mr. Karl Howell | male | 26.0 | 0 | 0 | 111369 | 30.00 | C148 | C | . 890 891 | 0 | 3 | Dooley, Mr. Patrick | male | 32.0 | 0 | 0 | 370376 | 7.75 | NaN | Q | . train_df.info() print(&#39;_&#39;*40) test_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): # Column Non-Null Count Dtype -- -- 0 PassengerId 891 non-null int64 1 Survived 891 non-null int64 2 Pclass 891 non-null int64 3 Name 891 non-null object 4 Sex 891 non-null object 5 Age 714 non-null float64 6 SibSp 891 non-null int64 7 Parch 891 non-null int64 8 Ticket 891 non-null object 9 Fare 891 non-null float64 10 Cabin 204 non-null object 11 Embarked 889 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 83.7+ KB ________________________________________ &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 418 entries, 0 to 417 Data columns (total 11 columns): # Column Non-Null Count Dtype -- -- 0 PassengerId 418 non-null int64 1 Pclass 418 non-null int64 2 Name 418 non-null object 3 Sex 418 non-null object 4 Age 332 non-null float64 5 SibSp 418 non-null int64 6 Parch 418 non-null int64 7 Ticket 418 non-null object 8 Fare 417 non-null float64 9 Cabin 91 non-null object 10 Embarked 418 non-null object dtypes: float64(2), int64(4), object(5) memory usage: 36.0+ KB . train_df.describe() # Review survived rate using `percentiles=[.61, .62]` knowing our problem description mentions 38% survival rate. # Review Parch distribution using `percentiles=[.75, .8]` # SibSp distribution `[.68, .69]` # Age and Fare `[.1, .2, .3, .4, .5, .6, .7, .8, .9, .99]` . PassengerId Survived Pclass Age SibSp Parch Fare . count 891.000000 | 891.000000 | 891.000000 | 714.000000 | 891.000000 | 891.000000 | 891.000000 | . mean 446.000000 | 0.383838 | 2.308642 | 29.699118 | 0.523008 | 0.381594 | 32.204208 | . std 257.353842 | 0.486592 | 0.836071 | 14.526497 | 1.102743 | 0.806057 | 49.693429 | . min 1.000000 | 0.000000 | 1.000000 | 0.420000 | 0.000000 | 0.000000 | 0.000000 | . 25% 223.500000 | 0.000000 | 2.000000 | 20.125000 | 0.000000 | 0.000000 | 7.910400 | . 50% 446.000000 | 0.000000 | 3.000000 | 28.000000 | 0.000000 | 0.000000 | 14.454200 | . 75% 668.500000 | 1.000000 | 3.000000 | 38.000000 | 1.000000 | 0.000000 | 31.000000 | . max 891.000000 | 1.000000 | 3.000000 | 80.000000 | 8.000000 | 6.000000 | 512.329200 | . train_df.describe(include=[&#39;O&#39;]) . Name Sex Ticket Cabin Embarked . count 891 | 891 | 891 | 204 | 889 | . unique 891 | 2 | 681 | 147 | 3 | . top Jussila, Mr. Eiriik | male | 347082 | B96 B98 | S | . freq 1 | 577 | 7 | 4 | 644 | . Checking correlation between features and the survival rate. . train_df[[&#39;Pclass&#39;, &#39;Survived&#39;]].groupby([&#39;Pclass&#39;], as_index=False).mean().sort_values(by=&#39;Survived&#39;, ascending=False) . Pclass Survived . 0 1 | 0.629630 | . 1 2 | 0.472826 | . 2 3 | 0.242363 | . train_df[[&quot;Sex&quot;, &quot;Survived&quot;]].groupby([&#39;Sex&#39;], as_index=False).mean().sort_values(by=&#39;Survived&#39;, ascending=False) . Sex Survived . 0 female | 0.742038 | . 1 male | 0.188908 | . train_df[[&quot;SibSp&quot;, &quot;Survived&quot;]].groupby([&#39;SibSp&#39;], as_index=False).mean().sort_values(by=&#39;Survived&#39;, ascending=False) . SibSp Survived . 1 1 | 0.535885 | . 2 2 | 0.464286 | . 0 0 | 0.345395 | . 3 3 | 0.250000 | . 4 4 | 0.166667 | . 5 5 | 0.000000 | . 6 8 | 0.000000 | . train_df[[&quot;Parch&quot;, &quot;Survived&quot;]].groupby([&#39;Parch&#39;], as_index=False).mean().sort_values(by=&#39;Survived&#39;, ascending=False) . Parch Survived . 3 3 | 0.600000 | . 1 1 | 0.550847 | . 2 2 | 0.500000 | . 0 0 | 0.343658 | . 5 5 | 0.200000 | . 4 4 | 0.000000 | . 6 6 | 0.000000 | . Analysing by Visualizing Data. . g = sns.FacetGrid(train_df, col=&#39;Survived&#39;) g.map(plt.hist, &#39;Age&#39;, bins=20) . &lt;seaborn.axisgrid.FacetGrid at 0x1493651e760&gt; . grid = sns.FacetGrid(train_df, col=&#39;Survived&#39;, row=&#39;Pclass&#39;, size=2.2, aspect=1.6) grid.map(plt.hist, &#39;Age&#39;, alpha=.5, bins=20) grid.add_legend(); . C: Users Shekhu anaconda3 lib site-packages seaborn axisgrid.py:243: UserWarning: The `size` parameter has been renamed to `height`; please update your code. warnings.warn(msg, UserWarning) . Our Obsvations are as follows. . a) Female passengers had much better survival rate than males. . b) Exception in Embarked=C where males had higher survival rate. This could be a correlation between Pclass and Embarked and in turn Pclass and Survived, not necessarily direct correlation between Embarked and Survived. . c) Males had better survival rate in Pclass=3 when compared with Pclass=2 for C and Q ports. . d) Ports of embarkation have varying survival rates for Pclass=3 and among male passengers. . grid = sns.FacetGrid(train_df, row=&#39;Embarked&#39;, size=2.2, aspect=1.6) grid.map(sns.pointplot, &#39;Pclass&#39;, &#39;Survived&#39;, &#39;Sex&#39;, palette=&#39;deep&#39;) grid.add_legend() . C: Users Shekhu anaconda3 lib site-packages seaborn axisgrid.py:723: UserWarning: Using the pointplot function without specifying `order` is likely to produce an incorrect plot. warnings.warn(warning) C: Users Shekhu anaconda3 lib site-packages seaborn axisgrid.py:728: UserWarning: Using the pointplot function without specifying `hue_order` is likely to produce an incorrect plot. warnings.warn(warning) . &lt;seaborn.axisgrid.FacetGrid at 0x14937139d60&gt; . grid = sns.FacetGrid(train_df, row=&#39;Embarked&#39;, col=&#39;Survived&#39;, size=2.2, aspect=1.6) grid.map(sns.barplot, &#39;Sex&#39;, &#39;Fare&#39;, alpha=.5, ci=None) grid.add_legend() . C: Users Shekhu anaconda3 lib site-packages seaborn axisgrid.py:243: UserWarning: The `size` parameter has been renamed to `height`; please update your code. warnings.warn(msg, UserWarning) C: Users Shekhu anaconda3 lib site-packages seaborn axisgrid.py:723: UserWarning: Using the barplot function without specifying `order` is likely to produce an incorrect plot. warnings.warn(warning) . &lt;seaborn.axisgrid.FacetGrid at 0x14937257bb0&gt; . Dropping some Features. . print(&quot;Before&quot;, train_df.shape, test_df.shape, combine[0].shape, combine[1].shape) train_df = train_df.drop([&#39;Ticket&#39;, &#39;Cabin&#39;], axis=1) test_df = test_df.drop([&#39;Ticket&#39;, &#39;Cabin&#39;], axis=1) combine = [train_df, test_df] &quot;After&quot;, train_df.shape, test_df.shape, combine[0].shape, combine[1].shape . Before (891, 12) (418, 11) (891, 12) (418, 11) . (&#39;After&#39;, (891, 10), (418, 9), (891, 10), (418, 9)) . We want to analyze if Name feature can be engineered to extract titles and test correlation between titles and survival, before dropping Name and PassengerId features. . for dataset in combine: dataset[&#39;Title&#39;] = dataset.Name.str.extract(&#39; ([A-Za-z]+) .&#39;, expand=False) pd.crosstab(train_df[&#39;Title&#39;], train_df[&#39;Sex&#39;]) . Sex female male . Title . Capt 0 | 1 | . Col 0 | 2 | . Countess 1 | 0 | . Don 0 | 1 | . Dr 1 | 6 | . Jonkheer 0 | 1 | . Lady 1 | 0 | . Major 0 | 2 | . Master 0 | 40 | . Miss 182 | 0 | . Mlle 2 | 0 | . Mme 1 | 0 | . Mr 0 | 517 | . Mrs 125 | 0 | . Ms 1 | 0 | . Rev 0 | 6 | . Sir 0 | 1 | . for dataset in combine: dataset[&#39;Title&#39;] = dataset[&#39;Title&#39;].replace([&#39;Lady&#39;, &#39;Countess&#39;,&#39;Capt&#39;, &#39;Col&#39;, &#39;Don&#39;, &#39;Dr&#39;, &#39;Major&#39;, &#39;Rev&#39;, &#39;Sir&#39;, &#39;Jonkheer&#39;, &#39;Dona&#39;], &#39;Rare&#39;) dataset[&#39;Title&#39;] = dataset[&#39;Title&#39;].replace(&#39;Mlle&#39;, &#39;Miss&#39;) dataset[&#39;Title&#39;] = dataset[&#39;Title&#39;].replace(&#39;Ms&#39;, &#39;Miss&#39;) dataset[&#39;Title&#39;] = dataset[&#39;Title&#39;].replace(&#39;Mme&#39;, &#39;Mrs&#39;) train_df[[&#39;Title&#39;, &#39;Survived&#39;]].groupby([&#39;Title&#39;], as_index=False).mean() . Title Survived . 0 Master | 0.575000 | . 1 Miss | 0.702703 | . 2 Mr | 0.156673 | . 3 Mrs | 0.793651 | . 4 Rare | 0.347826 | . title_mapping = {&quot;Mr&quot;: 1, &quot;Miss&quot;: 2, &quot;Mrs&quot;: 3, &quot;Master&quot;: 4, &quot;Rare&quot;: 5} for dataset in combine: dataset[&#39;Title&#39;] = dataset[&#39;Title&#39;].map(title_mapping) dataset[&#39;Title&#39;] = dataset[&#39;Title&#39;].fillna(0) train_df.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Fare Embarked Title . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | 7.2500 | S | 1 | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | 71.2833 | C | 3 | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | 7.9250 | S | 2 | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 53.1000 | S | 3 | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 8.0500 | S | 1 | . train_df = train_df.drop([&#39;Name&#39;, &#39;PassengerId&#39;], axis=1) test_df = test_df.drop([&#39;Name&#39;], axis=1) combine = [train_df, test_df] train_df.shape, test_df.shape . ((891, 9), (418, 9)) . for dataset in combine: dataset[&#39;Sex&#39;] = dataset[&#39;Sex&#39;].map( {&#39;female&#39;: 1, &#39;male&#39;: 0} ).astype(int) train_df.head() . Survived Pclass Sex Age SibSp Parch Fare Embarked Title . 0 0 | 3 | 0 | 22.0 | 1 | 0 | 7.2500 | S | 1 | . 1 1 | 1 | 1 | 38.0 | 1 | 0 | 71.2833 | C | 3 | . 2 1 | 3 | 1 | 26.0 | 0 | 0 | 7.9250 | S | 2 | . 3 1 | 1 | 1 | 35.0 | 1 | 0 | 53.1000 | S | 3 | . 4 0 | 3 | 0 | 35.0 | 0 | 0 | 8.0500 | S | 1 | . grid = sns.FacetGrid(train_df, row=&#39;Pclass&#39;, col=&#39;Sex&#39;, size=2.2, aspect=1.6) grid.map(plt.hist, &#39;Age&#39;, alpha=.5, bins=20) grid.add_legend() . C: Users Shekhu anaconda3 lib site-packages seaborn axisgrid.py:243: UserWarning: The `size` parameter has been renamed to `height`; please update your code. warnings.warn(msg, UserWarning) . &lt;seaborn.axisgrid.FacetGrid at 0x149373ff2b0&gt; . guess_ages = np.zeros((2,3)) guess_ages . array([[0., 0., 0.], [0., 0., 0.]]) . for dataset in combine: for i in range(0, 2): for j in range(0, 3): guess_df = dataset[(dataset[&#39;Sex&#39;] == i) &amp; (dataset[&#39;Pclass&#39;] == j+1)][&#39;Age&#39;].dropna() # age_mean = guess_df.mean() # age_std = guess_df.std() # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std) age_guess = guess_df.median() # Convert random age float to nearest .5 age guess_ages[i,j] = int( age_guess/0.5 + 0.5 ) * 0.5 for i in range(0, 2): for j in range(0, 3): dataset.loc[ (dataset.Age.isnull()) &amp; (dataset.Sex == i) &amp; (dataset.Pclass == j+1), &#39;Age&#39;] = guess_ages[i,j] dataset[&#39;Age&#39;] = dataset[&#39;Age&#39;].astype(int) train_df.head() . Survived Pclass Sex Age SibSp Parch Fare Embarked Title . 0 0 | 3 | 0 | 22 | 1 | 0 | 7.2500 | S | 1 | . 1 1 | 1 | 1 | 38 | 1 | 0 | 71.2833 | C | 3 | . 2 1 | 3 | 1 | 26 | 0 | 0 | 7.9250 | S | 2 | . 3 1 | 1 | 1 | 35 | 1 | 0 | 53.1000 | S | 3 | . 4 0 | 3 | 0 | 35 | 0 | 0 | 8.0500 | S | 1 | . train_df[&#39;AgeBand&#39;] = pd.cut(train_df[&#39;Age&#39;], 5) train_df[[&#39;AgeBand&#39;, &#39;Survived&#39;]].groupby([&#39;AgeBand&#39;], as_index=False).mean().sort_values(by=&#39;AgeBand&#39;, ascending=True) . AgeBand Survived . 0 (-0.08, 16.0] | 0.550000 | . 1 (16.0, 32.0] | 0.337374 | . 2 (32.0, 48.0] | 0.412037 | . 3 (48.0, 64.0] | 0.434783 | . 4 (64.0, 80.0] | 0.090909 | . for dataset in combine: dataset.loc[ dataset[&#39;Age&#39;] &lt;= 16, &#39;Age&#39;] = 0 dataset.loc[(dataset[&#39;Age&#39;] &gt; 16) &amp; (dataset[&#39;Age&#39;] &lt;= 32), &#39;Age&#39;] = 1 dataset.loc[(dataset[&#39;Age&#39;] &gt; 32) &amp; (dataset[&#39;Age&#39;] &lt;= 48), &#39;Age&#39;] = 2 dataset.loc[(dataset[&#39;Age&#39;] &gt; 48) &amp; (dataset[&#39;Age&#39;] &lt;= 64), &#39;Age&#39;] = 3 dataset.loc[ dataset[&#39;Age&#39;] &gt; 64, &#39;Age&#39;] train_df.head() . Survived Pclass Sex Age SibSp Parch Fare Embarked Title AgeBand . 0 0 | 3 | 0 | 1 | 1 | 0 | 7.2500 | S | 1 | (16.0, 32.0] | . 1 1 | 1 | 1 | 2 | 1 | 0 | 71.2833 | C | 3 | (32.0, 48.0] | . 2 1 | 3 | 1 | 1 | 0 | 0 | 7.9250 | S | 2 | (16.0, 32.0] | . 3 1 | 1 | 1 | 2 | 1 | 0 | 53.1000 | S | 3 | (32.0, 48.0] | . 4 0 | 3 | 0 | 2 | 0 | 0 | 8.0500 | S | 1 | (32.0, 48.0] | . train_df = train_df.drop([&#39;AgeBand&#39;], axis=1) combine = [train_df, test_df] train_df.head() . Survived Pclass Sex Age SibSp Parch Fare Embarked Title . 0 0 | 3 | 0 | 1 | 1 | 0 | 7.2500 | S | 1 | . 1 1 | 1 | 1 | 2 | 1 | 0 | 71.2833 | C | 3 | . 2 1 | 3 | 1 | 1 | 0 | 0 | 7.9250 | S | 2 | . 3 1 | 1 | 1 | 2 | 1 | 0 | 53.1000 | S | 3 | . 4 0 | 3 | 0 | 2 | 0 | 0 | 8.0500 | S | 1 | . for dataset in combine: dataset[&#39;FamilySize&#39;] = dataset[&#39;SibSp&#39;] + dataset[&#39;Parch&#39;] + 1 train_df[[&#39;FamilySize&#39;, &#39;Survived&#39;]].groupby([&#39;FamilySize&#39;], as_index=False).mean().sort_values(by=&#39;Survived&#39;, ascending=False) . FamilySize Survived . 3 4 | 0.724138 | . 2 3 | 0.578431 | . 1 2 | 0.552795 | . 6 7 | 0.333333 | . 0 1 | 0.303538 | . 4 5 | 0.200000 | . 5 6 | 0.136364 | . 7 8 | 0.000000 | . 8 11 | 0.000000 | . for dataset in combine: dataset[&#39;IsAlone&#39;] = 0 dataset.loc[dataset[&#39;FamilySize&#39;] == 1, &#39;IsAlone&#39;] = 1 train_df[[&#39;IsAlone&#39;, &#39;Survived&#39;]].groupby([&#39;IsAlone&#39;], as_index=False).mean() . IsAlone Survived . 0 0 | 0.505650 | . 1 1 | 0.303538 | . train_df = train_df.drop([&#39;Parch&#39;, &#39;SibSp&#39;, &#39;FamilySize&#39;], axis=1) test_df = test_df.drop([&#39;Parch&#39;, &#39;SibSp&#39;, &#39;FamilySize&#39;], axis=1) combine = [train_df, test_df] train_df.head() . Survived Pclass Sex Age Fare Embarked Title IsAlone . 0 0 | 3 | 0 | 1 | 7.2500 | S | 1 | 0 | . 1 1 | 1 | 1 | 2 | 71.2833 | C | 3 | 0 | . 2 1 | 3 | 1 | 1 | 7.9250 | S | 2 | 1 | . 3 1 | 1 | 1 | 2 | 53.1000 | S | 3 | 0 | . 4 0 | 3 | 0 | 2 | 8.0500 | S | 1 | 1 | . for dataset in combine: dataset[&#39;Age*Class&#39;] = dataset.Age * dataset.Pclass train_df.loc[:, [&#39;Age*Class&#39;, &#39;Age&#39;, &#39;Pclass&#39;]].head(10) . Age*Class Age Pclass . 0 3 | 1 | 3 | . 1 2 | 2 | 1 | . 2 3 | 1 | 3 | . 3 2 | 2 | 1 | . 4 6 | 2 | 3 | . 5 3 | 1 | 3 | . 6 3 | 3 | 1 | . 7 0 | 0 | 3 | . 8 3 | 1 | 3 | . 9 0 | 0 | 2 | . freq_port = train_df.Embarked.dropna().mode()[0] freq_port . &#39;S&#39; . for dataset in combine: dataset[&#39;Embarked&#39;] = dataset[&#39;Embarked&#39;].fillna(freq_port) train_df[[&#39;Embarked&#39;, &#39;Survived&#39;]].groupby([&#39;Embarked&#39;], as_index=False).mean().sort_values(by=&#39;Survived&#39;, ascending=False) . Embarked Survived . 0 C | 0.553571 | . 1 Q | 0.389610 | . 2 S | 0.339009 | . for dataset in combine: dataset[&#39;Embarked&#39;] = dataset[&#39;Embarked&#39;].map( {&#39;S&#39;: 0, &#39;C&#39;: 1, &#39;Q&#39;: 2} ).astype(int) train_df.head() . Survived Pclass Sex Age Fare Embarked Title IsAlone Age*Class . 0 0 | 3 | 0 | 1 | 7.2500 | 0 | 1 | 0 | 3 | . 1 1 | 1 | 1 | 2 | 71.2833 | 1 | 3 | 0 | 2 | . 2 1 | 3 | 1 | 1 | 7.9250 | 0 | 2 | 1 | 3 | . 3 1 | 1 | 1 | 2 | 53.1000 | 0 | 3 | 0 | 2 | . 4 0 | 3 | 0 | 2 | 8.0500 | 0 | 1 | 1 | 6 | . test_df[&#39;Fare&#39;].fillna(test_df[&#39;Fare&#39;].dropna().median(), inplace=True) test_df.head() . PassengerId Pclass Sex Age Fare Embarked Title IsAlone Age*Class . 0 892 | 3 | 0 | 2 | 7.8292 | 2 | 1 | 1 | 6 | . 1 893 | 3 | 1 | 2 | 7.0000 | 0 | 3 | 0 | 6 | . 2 894 | 2 | 0 | 3 | 9.6875 | 2 | 1 | 1 | 6 | . 3 895 | 3 | 0 | 1 | 8.6625 | 0 | 1 | 1 | 3 | . 4 896 | 3 | 1 | 1 | 12.2875 | 0 | 3 | 0 | 3 | . train_df[&#39;FareBand&#39;] = pd.qcut(train_df[&#39;Fare&#39;], 4) train_df[[&#39;FareBand&#39;, &#39;Survived&#39;]].groupby([&#39;FareBand&#39;], as_index=False).mean().sort_values(by=&#39;FareBand&#39;, ascending=True) . FareBand Survived . 0 (-0.001, 7.91] | 0.197309 | . 1 (7.91, 14.454] | 0.303571 | . 2 (14.454, 31.0] | 0.454955 | . 3 (31.0, 512.329] | 0.581081 | . for dataset in combine: dataset.loc[ dataset[&#39;Fare&#39;] &lt;= 7.91, &#39;Fare&#39;] = 0 dataset.loc[(dataset[&#39;Fare&#39;] &gt; 7.91) &amp; (dataset[&#39;Fare&#39;] &lt;= 14.454), &#39;Fare&#39;] = 1 dataset.loc[(dataset[&#39;Fare&#39;] &gt; 14.454) &amp; (dataset[&#39;Fare&#39;] &lt;= 31), &#39;Fare&#39;] = 2 dataset.loc[ dataset[&#39;Fare&#39;] &gt; 31, &#39;Fare&#39;] = 3 dataset[&#39;Fare&#39;] = dataset[&#39;Fare&#39;].astype(int) train_df = train_df.drop([&#39;FareBand&#39;], axis=1) combine = [train_df, test_df] train_df.head(10) . Survived Pclass Sex Age Fare Embarked Title IsAlone Age*Class . 0 0 | 3 | 0 | 1 | 0 | 0 | 1 | 0 | 3 | . 1 1 | 1 | 1 | 2 | 3 | 1 | 3 | 0 | 2 | . 2 1 | 3 | 1 | 1 | 1 | 0 | 2 | 1 | 3 | . 3 1 | 1 | 1 | 2 | 3 | 0 | 3 | 0 | 2 | . 4 0 | 3 | 0 | 2 | 1 | 0 | 1 | 1 | 6 | . 5 0 | 3 | 0 | 1 | 1 | 2 | 1 | 1 | 3 | . 6 0 | 1 | 0 | 3 | 3 | 0 | 1 | 1 | 3 | . 7 0 | 3 | 0 | 0 | 2 | 0 | 4 | 0 | 0 | . 8 1 | 3 | 1 | 1 | 1 | 0 | 3 | 0 | 3 | . 9 1 | 2 | 1 | 0 | 2 | 1 | 3 | 0 | 0 | . test_df.head(10) . PassengerId Pclass Sex Age Fare Embarked Title IsAlone Age*Class . 0 892 | 3 | 0 | 2 | 0 | 2 | 1 | 1 | 6 | . 1 893 | 3 | 1 | 2 | 0 | 0 | 3 | 0 | 6 | . 2 894 | 2 | 0 | 3 | 1 | 2 | 1 | 1 | 6 | . 3 895 | 3 | 0 | 1 | 1 | 0 | 1 | 1 | 3 | . 4 896 | 3 | 1 | 1 | 1 | 0 | 3 | 0 | 3 | . 5 897 | 3 | 0 | 0 | 1 | 0 | 1 | 1 | 0 | . 6 898 | 3 | 1 | 1 | 0 | 2 | 2 | 1 | 3 | . 7 899 | 2 | 0 | 1 | 2 | 0 | 1 | 0 | 2 | . 8 900 | 3 | 1 | 1 | 0 | 1 | 3 | 1 | 3 | . 9 901 | 3 | 0 | 1 | 2 | 0 | 1 | 0 | 3 | . Running some Models . X_train = train_df.drop(&quot;Survived&quot;, axis=1) Y_train = train_df[&quot;Survived&quot;] X_test = test_df.drop(&quot;PassengerId&quot;, axis=1).copy() X_train.shape, Y_train.shape, X_test.shape . ((891, 8), (891,), (418, 8)) . Logistic Regression Score . logreg = LogisticRegression() logreg.fit(X_train, Y_train) Y_pred = logreg.predict(X_test) acc_log = round(logreg.score(X_train, Y_train) * 100, 2) acc_log . 80.36 . coeff_df = pd.DataFrame(train_df.columns.delete(0)) coeff_df.columns = [&#39;Feature&#39;] coeff_df[&quot;Correlation&quot;] = pd.Series(logreg.coef_[0]) coeff_df.sort_values(by=&#39;Correlation&#39;, ascending=False) . Feature Correlation . 1 Sex | 2.201619 | . 5 Title | 0.397888 | . 2 Age | 0.287011 | . 4 Embarked | 0.261473 | . 6 IsAlone | 0.126553 | . 3 Fare | -0.086655 | . 7 Age*Class | -0.311069 | . 0 Pclass | -0.750700 | . Support Vector Machine (S.V.M.) . svc = SVC() svc.fit(X_train, Y_train) Y_pred = svc.predict(X_test) acc_svc = round(svc.score(X_train, Y_train) * 100, 2) acc_svc . 78.23 . K-Nearest Neighbours (k-NN) . knn = KNeighborsClassifier(n_neighbors = 3) knn.fit(X_train, Y_train) Y_pred = knn.predict(X_test) acc_knn = round(knn.score(X_train, Y_train) * 100, 2) acc_knn . 84.74 . Gaussian Naive Bayes . gaussian = GaussianNB() gaussian.fit(X_train, Y_train) Y_pred = gaussian.predict(X_test) acc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2) acc_gaussian . 72.28 . Perceptron Algorithm . perceptron = Perceptron() perceptron.fit(X_train, Y_train) Y_pred = perceptron.predict(X_test) acc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2) acc_perceptron . 78.34 . Linear SVC . linear_svc = LinearSVC() linear_svc.fit(X_train, Y_train) Y_pred = linear_svc.predict(X_test) acc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2) acc_linear_svc . C: Users Shekhu anaconda3 lib site-packages sklearn svm _base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. warnings.warn(&#34;Liblinear failed to converge, increase &#34; . 79.12 . Stochastic Gradient Descent . sgd = SGDClassifier() sgd.fit(X_train, Y_train) Y_pred = sgd.predict(X_test) acc_sgd = round(sgd.score(X_train, Y_train) * 100, 2) acc_sgd . 73.4 . Decision Tree . decision_tree = DecisionTreeClassifier() decision_tree.fit(X_train, Y_train) Y_pred = decision_tree.predict(X_test) acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2) acc_decision_tree . 86.76 . Using Random Forest Algorithm . random_forest = RandomForestClassifier(n_estimators=100) random_forest.fit(X_train, Y_train) Y_pred = random_forest.predict(X_test) random_forest.score(X_train, Y_train) acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2) acc_random_forest . 86.76 . Model Evaluation . models = pd.DataFrame({ &#39;Model&#39;: [&#39;Support Vector Machines&#39;, &#39;KNN&#39;, &#39;Logistic Regression&#39;, &#39;Random Forest&#39;, &#39;Naive Bayes&#39;, &#39;Perceptron&#39;, &#39;Stochastic Gradient Decent&#39;, &#39;Linear SVC&#39;, &#39;Decision Tree&#39;], &#39;Score&#39;: [acc_svc, acc_knn, acc_log, acc_random_forest, acc_gaussian, acc_perceptron, acc_sgd, acc_linear_svc, acc_decision_tree]}) models.sort_values(by=&#39;Score&#39;, ascending=False) . Model Score . 3 Random Forest | 86.76 | . 8 Decision Tree | 86.76 | . 1 KNN | 84.74 | . 2 Logistic Regression | 80.36 | . 7 Linear SVC | 79.12 | . 5 Perceptron | 78.34 | . 0 Support Vector Machines | 78.23 | . 6 Stochastic Gradient Decent | 73.40 | . 4 Naive Bayes | 72.28 | . submission = pd.DataFrame({ &quot;PassengerId&quot;: test_df[&quot;PassengerId&quot;], &quot;Survived&quot;: Y_pred }) # submission.to_csv(&#39;../output/submission.csv&#39;, index=False) .",
            "url": "https://siddiquisuhail.github.io/delhi_aqi/2021/06/14/My-First-Post.html",
            "relUrl": "/2021/06/14/My-First-Post.html",
            "date": " • Jun 14, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://siddiquisuhail.github.io/delhi_aqi/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://siddiquisuhail.github.io/delhi_aqi/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://siddiquisuhail.github.io/delhi_aqi/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://siddiquisuhail.github.io/delhi_aqi/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}